{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 16: Adaptive LayerNorm (AlphaFold3)\n",
    "\n",
    "Adaptive LayerNorm (AdaLN) is a key component of the Diffusion Transformer in AlphaFold3. It modulates normalized features based on conditioning signals, following the approach from the DiT (Diffusion Transformer) paper.\n",
    "\n",
    "## Source Code Location\n",
    "- **File**: `AF3-Ref-src/alphafold3-official/src/alphafold3/model/network/diffusion_transformer.py`\n",
    "- **Function**: `adaptive_layernorm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### Standard LayerNorm vs Adaptive LayerNorm\n",
    "\n",
    "**Standard LayerNorm:**\n",
    "```\n",
    "y = γ * (x - μ) / σ + β\n",
    "```\n",
    "where γ, β are learnable parameters.\n",
    "\n",
    "**Adaptive LayerNorm:**\n",
    "```\n",
    "y = scale(c) * LayerNorm(x) + bias(c)\n",
    "```\n",
    "where scale and bias are functions of conditioning c.\n",
    "\n",
    "### Why Adaptive?\n",
    "\n",
    "In diffusion models, the network needs to behave differently at different noise levels. AdaLN allows the conditioning (timestep, single representation) to modulate the entire layer's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, axis=-1, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Standard Layer Normalization.\n",
    "    \n",
    "    Normalizes along the last axis.\n",
    "    \"\"\"\n",
    "    mean = np.mean(x, axis=axis, keepdims=True)\n",
    "    var = np.var(x, axis=axis, keepdims=True)\n",
    "    return (x - mean) / np.sqrt(var + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_layer_norm(x, single_cond, c_out=None):\n",
    "    \"\"\"\n",
    "    Adaptive LayerNorm - Algorithm 16.\n",
    "    \n",
    "    Modulates normalized features based on conditioning.\n",
    "    \n",
    "    Args:\n",
    "        x: Input features [*, c]\n",
    "        single_cond: Conditioning signal [*, c_cond] or None\n",
    "        c_out: Output channels (defaults to x.shape[-1])\n",
    "    \n",
    "    Returns:\n",
    "        Modulated features\n",
    "    \"\"\"\n",
    "    c = x.shape[-1]\n",
    "    if c_out is None:\n",
    "        c_out = c\n",
    "    \n",
    "    print(f\"Adaptive LayerNorm\")\n",
    "    print(f\"  Input: {x.shape}\")\n",
    "    \n",
    "    # Step 1: Standard LayerNorm (without learnable params)\n",
    "    x_norm = layer_norm(x)\n",
    "    print(f\"  Step 1: LayerNorm applied\")\n",
    "    \n",
    "    if single_cond is None:\n",
    "        print(f\"  No conditioning - returning normalized\")\n",
    "        return x_norm\n",
    "    \n",
    "    c_cond = single_cond.shape[-1]\n",
    "    print(f\"  Conditioning: {single_cond.shape}\")\n",
    "    \n",
    "    # Step 2: Normalize conditioning\n",
    "    single_cond_norm = layer_norm(single_cond)\n",
    "    print(f\"  Step 2: Conditioning normalized\")\n",
    "    \n",
    "    # Step 3: Compute scale (initialized near 1 via sigmoid)\n",
    "    W_scale = np.random.randn(c_cond, c_out) * 0.02\n",
    "    b_scale = np.zeros(c_out)  # Bias to start near sigmoid(0) = 0.5, doubled = 1\n",
    "    \n",
    "    scale_logits = single_cond_norm @ W_scale + b_scale\n",
    "    scale = 1.0 / (1.0 + np.exp(-scale_logits))  # sigmoid\n",
    "    print(f\"  Step 3: Scale computed (mean={scale.mean():.3f})\")\n",
    "    \n",
    "    # Step 4: Compute bias\n",
    "    W_bias = np.random.randn(c_cond, c_out) * 0.02\n",
    "    bias = single_cond_norm @ W_bias\n",
    "    print(f\"  Step 4: Bias computed (mean={bias.mean():.3f})\")\n",
    "    \n",
    "    # Step 5: Apply modulation\n",
    "    output = scale * x_norm + bias\n",
    "    print(f\"  Step 5: Modulation applied\")\n",
    "    print(f\"  Output: {output.shape}\")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_zero_init(x, c_out, single_cond, name=''):\n",
    "    \"\"\"\n",
    "    Adaptive Zero Init (AdaLN-Zero).\n",
    "    \n",
    "    Like AdaLN but with zero-initialized conditioning path.\n",
    "    Used for residual connections to start as identity.\n",
    "    \n",
    "    Args:\n",
    "        x: Input after processing [*, c]\n",
    "        c_out: Output dimension\n",
    "        single_cond: Conditioning signal\n",
    "    \n",
    "    Returns:\n",
    "        Scaled output (starts near zero)\n",
    "    \"\"\"\n",
    "    c = x.shape[-1]\n",
    "    \n",
    "    print(f\"Adaptive Zero Init\")\n",
    "    print(f\"  Input: {x.shape}\")\n",
    "    \n",
    "    # Linear projection\n",
    "    W = np.random.randn(c, c_out) * (c ** -0.5)\n",
    "    output = x @ W\n",
    "    \n",
    "    if single_cond is None:\n",
    "        # Zero-initialized final projection (small output)\n",
    "        output = output * 0.01\n",
    "        print(f\"  No conditioning - small init\")\n",
    "    else:\n",
    "        c_cond = single_cond.shape[-1]\n",
    "        \n",
    "        # Scale initialized with bias=-2 so sigmoid(-2) ≈ 0.12\n",
    "        W_scale = np.random.randn(c_cond, c_out) * 0.02\n",
    "        b_scale = np.full(c_out, -2.0)  # Initialize to produce ~0.1 scaling\n",
    "        \n",
    "        scale_logits = single_cond @ W_scale + b_scale\n",
    "        scale = 1.0 / (1.0 + np.exp(-scale_logits))\n",
    "        \n",
    "        output = output * scale\n",
    "        print(f\"  Conditioning applied (scale mean={scale.mean():.3f})\")\n",
    "    \n",
    "    print(f\"  Output: {output.shape}\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Basic Adaptive LayerNorm\n",
    "print(\"Test 1: Basic Adaptive LayerNorm\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "N = 32\n",
    "c = 128\n",
    "c_cond = 64\n",
    "\n",
    "x = np.random.randn(N, c).astype(np.float32) * 2  # Large variance\n",
    "single_cond = np.random.randn(N, c_cond).astype(np.float32)\n",
    "\n",
    "output = adaptive_layer_norm(x, single_cond)\n",
    "\n",
    "print(f\"\\nInput std: {x.std():.3f}\")\n",
    "print(f\"Output std: {output.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Without conditioning (standard LayerNorm)\n",
    "print(\"\\nTest 2: Without Conditioning\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "x = np.random.randn(32, 128)\n",
    "output = adaptive_layer_norm(x, None)\n",
    "\n",
    "# Should be normalized\n",
    "print(f\"\\nOutput mean: {output.mean(axis=-1).mean():.6f} (expected ~0)\")\n",
    "print(f\"Output std: {output.std(axis=-1).mean():.6f} (expected ~1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Adaptive Zero Init\n",
    "print(\"\\nTest 3: Adaptive Zero Init\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "x = np.random.randn(32, 128)\n",
    "single_cond = np.random.randn(32, 64)\n",
    "\n",
    "output = adaptive_zero_init(x, c_out=128, single_cond=single_cond)\n",
    "\n",
    "print(f\"\\nInput norm: {np.linalg.norm(x):.3f}\")\n",
    "print(f\"Output norm: {np.linalg.norm(output):.3f} (should be much smaller)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Effect of different conditioning\n",
    "print(\"\\nTest 4: Effect of Different Conditioning\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "N = 16\n",
    "c = 64\n",
    "c_cond = 32\n",
    "\n",
    "x = np.random.randn(N, c)\n",
    "\n",
    "# Different conditioning signals\n",
    "cond_1 = np.ones((N, c_cond))  # Uniform positive\n",
    "cond_2 = -np.ones((N, c_cond))  # Uniform negative\n",
    "cond_3 = np.random.randn(N, c_cond)  # Random\n",
    "\n",
    "out_1 = adaptive_layer_norm(x, cond_1)\n",
    "out_2 = adaptive_layer_norm(x, cond_2)\n",
    "out_3 = adaptive_layer_norm(x, cond_3)\n",
    "\n",
    "print(f\"\\nConditioning 1 (positive): output mean={out_1.mean():.3f}, std={out_1.std():.3f}\")\n",
    "print(f\"Conditioning 2 (negative): output mean={out_2.mean():.3f}, std={out_2.std():.3f}\")\n",
    "print(f\"Conditioning 3 (random):   output mean={out_3.mean():.3f}, std={out_3.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification: Key Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verification: Key Properties\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "N = 32\n",
    "c = 64\n",
    "c_cond = 32\n",
    "\n",
    "x = np.random.randn(N, c)\n",
    "cond = np.random.randn(N, c_cond)\n",
    "\n",
    "output = adaptive_layer_norm(x, cond)\n",
    "\n",
    "# Property 1: Shape preserved\n",
    "shape_preserved = output.shape == x.shape\n",
    "print(f\"Property 1 - Shape preserved: {shape_preserved}\")\n",
    "\n",
    "# Property 2: Finite output\n",
    "all_finite = np.isfinite(output).all()\n",
    "print(f\"Property 2 - All finite: {all_finite}\")\n",
    "\n",
    "# Property 3: Without conditioning, output is normalized\n",
    "output_no_cond = adaptive_layer_norm(x, None)\n",
    "is_normalized = np.allclose(output_no_cond.mean(axis=-1), 0, atol=1e-5) and \\\n",
    "                np.allclose(output_no_cond.std(axis=-1), 1, atol=1e-5)\n",
    "print(f\"Property 3 - No-cond is normalized: {is_normalized}\")\n",
    "\n",
    "# Property 4: AdaLN-Zero starts small\n",
    "x_proc = np.random.randn(N, c)\n",
    "output_zero = adaptive_zero_init(x_proc, c, cond)\n",
    "is_small = np.linalg.norm(output_zero) < np.linalg.norm(x_proc) * 0.5\n",
    "print(f\"Property 4 - AdaLN-Zero is small: {is_small}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code Reference\n",
    "\n",
    "```python\n",
    "# From AF3-Ref-src/alphafold3-official/src/alphafold3/model/network/diffusion_transformer.py\n",
    "\n",
    "def adaptive_layernorm(x, single_cond, name):\n",
    "  \"\"\"Adaptive LayerNorm.\"\"\"\n",
    "  # Adopted from Scalable Diffusion Models with Transformers\n",
    "  # https://arxiv.org/abs/2212.09748\n",
    "  if single_cond is None:\n",
    "    x = hm.LayerNorm(name=f'{name}layer_norm', use_fast_variance=False)(x)\n",
    "  else:\n",
    "    x = hm.LayerNorm(\n",
    "        name=f'{name}layer_norm',\n",
    "        use_fast_variance=False,\n",
    "        create_scale=False,\n",
    "        create_offset=False,\n",
    "    )(x)\n",
    "    single_cond = hm.LayerNorm(\n",
    "        name=f'{name}single_cond_layer_norm',\n",
    "        use_fast_variance=False,\n",
    "        create_offset=False,\n",
    "    )(single_cond)\n",
    "    single_scale = hm.Linear(\n",
    "        x.shape[-1],\n",
    "        initializer='zeros',\n",
    "        use_bias=True,\n",
    "        name=f'{name}single_cond_scale',\n",
    "    )(single_cond)\n",
    "    single_bias = hm.Linear(\n",
    "        x.shape[-1], initializer='zeros', name=f'{name}single_cond_bias'\n",
    "    )(single_cond)\n",
    "    x = jax.nn.sigmoid(single_scale) * x + single_bias\n",
    "  return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "1. **Conditioning Modulation**: AdaLN allows the model to dynamically adjust layer behavior based on timestep and sequence information.\n",
    "\n",
    "2. **Scale via Sigmoid**: Using sigmoid for scale ensures it's always positive and bounded, preventing instability.\n",
    "\n",
    "3. **Zero Initialization**: The AdaLN-Zero variant initializes residual paths to near-zero, making early training more stable.\n",
    "\n",
    "4. **From DiT Paper**: This technique is adopted from \"Scalable Diffusion Models with Transformers\" (Peebles & Xie, 2022).\n",
    "\n",
    "5. **Per-Token Modulation**: Each token can receive different scale/bias based on its conditioning, enabling fine-grained control.\n",
    "\n",
    "6. **Replaces Class Conditioning**: In image diffusion, AdaLN often conditions on class labels; in AF3, it conditions on sequence features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
