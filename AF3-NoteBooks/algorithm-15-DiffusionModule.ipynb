{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 15: Diffusion Module (AlphaFold3)\n",
    "\n",
    "The Diffusion Module is the core structural prediction component of AlphaFold3, replacing AlphaFold2's IPA-based Structure Module. It uses a denoising diffusion process to generate 3D atomic coordinates.\n",
    "\n",
    "## Key Difference from AlphaFold2\n",
    "\n",
    "| Aspect | AlphaFold2 | AlphaFold3 |\n",
    "|--------|------------|------------|\n",
    "| Prediction | Direct backbone frames + torsions | Denoising diffusion on all atoms |\n",
    "| Output | Single structure | Ensemble of structures |\n",
    "| Iterations | 8 refinement steps | 200 diffusion steps |\n",
    "\n",
    "## Source Code Location\n",
    "- **File**: `AF3-Ref-src/alphafold3-official/src/alphafold3/model/network/diffusion_head.py`\n",
    "- **Class**: `DiffusionHead`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### Diffusion Process\n",
    "\n",
    "```\n",
    "Forward Process (Training):\n",
    "x₀ (clean) → x₁ → x₂ → ... → x_T (noise)\n",
    "           +ε   +ε        +ε\n",
    "\n",
    "Reverse Process (Inference):\n",
    "x_T (noise) → x_{T-1} → ... → x₁ → x₀ (predicted)\n",
    "            denoise    denoise   denoise\n",
    "```\n",
    "\n",
    "### AlphaFold3 Diffusion\n",
    "\n",
    "1. **Noise Schedule**: Defines how noise is added at each step\n",
    "2. **Conditioning**: Single and pair representations condition the denoising\n",
    "3. **Atom Cross Attention**: Aggregates information across atoms\n",
    "4. **Diffusion Transformer**: Processes noisy coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_schedule(num_steps=200, s=0.008):\n",
    "    \"\"\"\n",
    "    Cosine noise schedule for diffusion.\n",
    "    \n",
    "    Args:\n",
    "        num_steps: Number of diffusion steps\n",
    "        s: Small offset to prevent singularity\n",
    "    \n",
    "    Returns:\n",
    "        alphas_cumprod: Cumulative product of alphas\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, num_steps, num_steps + 1)\n",
    "    f_t = np.cos((t / num_steps + s) / (1 + s) * np.pi / 2) ** 2\n",
    "    alphas_cumprod = f_t / f_t[0]\n",
    "    return alphas_cumprod\n",
    "\n",
    "\n",
    "def add_noise(x0, t, alphas_cumprod):\n",
    "    \"\"\"\n",
    "    Add noise to clean coordinates at timestep t.\n",
    "    \n",
    "    x_t = sqrt(alpha_t) * x_0 + sqrt(1 - alpha_t) * epsilon\n",
    "    \"\"\"\n",
    "    alpha_t = alphas_cumprod[t]\n",
    "    noise = np.random.randn(*x0.shape)\n",
    "    x_t = np.sqrt(alpha_t) * x0 + np.sqrt(1 - alpha_t) * noise\n",
    "    return x_t, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, axis=-1, eps=1e-5):\n",
    "    \"\"\"Layer normalization.\"\"\"\n",
    "    mean = np.mean(x, axis=axis, keepdims=True)\n",
    "    var = np.var(x, axis=axis, keepdims=True)\n",
    "    return (x - mean) / np.sqrt(var + eps)\n",
    "\n",
    "\n",
    "def adaptive_layer_norm(x, single_cond):\n",
    "    \"\"\"\n",
    "    Adaptive LayerNorm (AdaLN) from DiT paper.\n",
    "    \n",
    "    Modulates normalized features based on conditioning.\n",
    "    \"\"\"\n",
    "    x_norm = layer_norm(x)\n",
    "    \n",
    "    if single_cond is None:\n",
    "        return x_norm\n",
    "    \n",
    "    single_cond_norm = layer_norm(single_cond)\n",
    "    \n",
    "    c = x.shape[-1]\n",
    "    W_scale = np.random.randn(single_cond.shape[-1], c) * 0.02\n",
    "    W_bias = np.random.randn(single_cond.shape[-1], c) * 0.02\n",
    "    \n",
    "    scale = 1.0 / (1.0 + np.exp(-single_cond_norm @ W_scale))  # sigmoid\n",
    "    bias = single_cond_norm @ W_bias\n",
    "    \n",
    "    return scale * x_norm + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_level_embedding(t, c=128, max_t=200):\n",
    "    \"\"\"\n",
    "    Sinusoidal embedding for noise level (timestep).\n",
    "    \n",
    "    Similar to positional encoding in Transformers.\n",
    "    \"\"\"\n",
    "    half_c = c // 2\n",
    "    freqs = np.exp(-np.log(10000.0) * np.arange(half_c) / half_c)\n",
    "    \n",
    "    # Normalize timestep to [0, 1]\n",
    "    t_normalized = t / max_t\n",
    "    \n",
    "    angles = t_normalized * freqs * 1000\n",
    "    embedding = np.concatenate([np.sin(angles), np.cos(angles)])\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_module_forward(x_t, t, single, pair, c=128):\n",
    "    \"\"\"\n",
    "    Simplified Diffusion Module forward pass.\n",
    "    \n",
    "    Args:\n",
    "        x_t: Noisy atom coordinates [N_atoms, 3]\n",
    "        t: Current timestep\n",
    "        single: Single representation [N_tokens, c_s]\n",
    "        pair: Pair representation [N_tokens, N_tokens, c_z]\n",
    "        c: Hidden dimension\n",
    "    \n",
    "    Returns:\n",
    "        Predicted noise [N_atoms, 3]\n",
    "    \"\"\"\n",
    "    N_atoms = x_t.shape[0]\n",
    "    N_tokens = single.shape[0]\n",
    "    c_s = single.shape[-1]\n",
    "    \n",
    "    print(f\"Diffusion Module Forward\")\n",
    "    print(f\"=\"*50)\n",
    "    print(f\"Atoms: {N_atoms}, Tokens: {N_tokens}\")\n",
    "    print(f\"Timestep: {t}\")\n",
    "    \n",
    "    # Step 1: Noise level embedding\n",
    "    t_emb = noise_level_embedding(t, c=c_s)\n",
    "    single_cond = single + t_emb[None, :]  # Broadcast to all tokens\n",
    "    print(f\"\\nStep 1: Noise embedding added to single\")\n",
    "    \n",
    "    # Step 2: Project coordinates to hidden dimension\n",
    "    W_in = np.random.randn(3, c) * 0.1\n",
    "    h = x_t @ W_in  # [N_atoms, c]\n",
    "    print(f\"Step 2: Coordinates projected: {h.shape}\")\n",
    "    \n",
    "    # Step 3: Adaptive LayerNorm with conditioning\n",
    "    # Simplified: use mean of single as conditioning\n",
    "    cond = single_cond.mean(axis=0)  # [c_s]\n",
    "    W_cond = np.random.randn(c_s, c) * 0.1\n",
    "    cond_proj = cond @ W_cond  # [c]\n",
    "    \n",
    "    h = adaptive_layer_norm(h, np.tile(cond_proj, (N_atoms, 1)))\n",
    "    print(f\"Step 3: Adaptive LayerNorm applied\")\n",
    "    \n",
    "    # Step 4: Simplified self-attention (would be Diffusion Transformer)\n",
    "    W_q = np.random.randn(c, c) * (c ** -0.5)\n",
    "    W_k = np.random.randn(c, c) * (c ** -0.5)\n",
    "    W_v = np.random.randn(c, c) * (c ** -0.5)\n",
    "    \n",
    "    q = h @ W_q\n",
    "    k = h @ W_k\n",
    "    v = h @ W_v\n",
    "    \n",
    "    attn = np.softmax(q @ k.T / np.sqrt(c), axis=-1) if hasattr(np, 'softmax') else \\\n",
    "           np.exp(q @ k.T / np.sqrt(c)) / np.exp(q @ k.T / np.sqrt(c)).sum(axis=-1, keepdims=True)\n",
    "    h = attn @ v\n",
    "    print(f\"Step 4: Self-attention computed\")\n",
    "    \n",
    "    # Step 5: MLP / Transition\n",
    "    W_up = np.random.randn(c, c * 4) * 0.1\n",
    "    W_down = np.random.randn(c * 4, c) * 0.1\n",
    "    h = h @ W_up\n",
    "    h = np.maximum(0, h)  # ReLU\n",
    "    h = h @ W_down\n",
    "    print(f\"Step 5: MLP transition applied\")\n",
    "    \n",
    "    # Step 6: Project back to 3D\n",
    "    W_out = np.random.randn(c, 3) * 0.02  # Small init for residual\n",
    "    noise_pred = h @ W_out\n",
    "    print(f\"Step 6: Output projected: {noise_pred.shape}\")\n",
    "    \n",
    "    return noise_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_diffusion(single, pair, N_atoms, num_steps=20):\n",
    "    \"\"\"\n",
    "    Sample structure using reverse diffusion process.\n",
    "    \n",
    "    Simplified version with fewer steps for demonstration.\n",
    "    \"\"\"\n",
    "    print(f\"\\nSampling with {num_steps} diffusion steps\")\n",
    "    print(f\"=\"*50)\n",
    "    \n",
    "    alphas_cumprod = get_noise_schedule(num_steps)\n",
    "    \n",
    "    # Start from pure noise\n",
    "    x_t = np.random.randn(N_atoms, 3) * 10  # Scale for Angstroms\n",
    "    print(f\"Initial noise std: {x_t.std():.2f}\")\n",
    "    \n",
    "    trajectory = [x_t.copy()]\n",
    "    \n",
    "    # Reverse diffusion\n",
    "    for t in range(num_steps, 0, -1):\n",
    "        # Predict noise\n",
    "        noise_pred = diffusion_module_forward(x_t, t, single, pair)\n",
    "        \n",
    "        # Compute alpha values\n",
    "        alpha_t = alphas_cumprod[t]\n",
    "        alpha_t_prev = alphas_cumprod[t - 1]\n",
    "        \n",
    "        # Predict x_0\n",
    "        x0_pred = (x_t - np.sqrt(1 - alpha_t) * noise_pred) / np.sqrt(alpha_t)\n",
    "        \n",
    "        # Compute x_{t-1}\n",
    "        if t > 1:\n",
    "            noise = np.random.randn(*x_t.shape)\n",
    "            sigma = np.sqrt((1 - alpha_t_prev) / (1 - alpha_t) * (1 - alpha_t / alpha_t_prev))\n",
    "            x_t = np.sqrt(alpha_t_prev) * x0_pred + \\\n",
    "                  np.sqrt(1 - alpha_t_prev - sigma**2) * noise_pred + \\\n",
    "                  sigma * noise\n",
    "        else:\n",
    "            x_t = x0_pred\n",
    "        \n",
    "        trajectory.append(x_t.copy())\n",
    "        \n",
    "        if t % 5 == 0:\n",
    "            print(f\"Step {t}: std = {x_t.std():.2f}\")\n",
    "    \n",
    "    print(f\"\\nFinal structure std: {x_t.std():.2f}\")\n",
    "    return x_t, trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Noise schedule visualization\n",
    "print(\"Test 1: Noise Schedule\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "alphas = get_noise_schedule(200)\n",
    "print(f\"Alpha at t=0: {alphas[0]:.4f} (clean)\")\n",
    "print(f\"Alpha at t=100: {alphas[100]:.4f}\")\n",
    "print(f\"Alpha at t=200: {alphas[200]:.4f} (noisy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Forward diffusion (adding noise)\n",
    "print(\"\\nTest 2: Forward Diffusion\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clean coordinates (simple helix)\n",
    "N_atoms = 50\n",
    "t_param = np.linspace(0, 4 * np.pi, N_atoms)\n",
    "x0 = np.stack([\n",
    "    t_param * 0.5,\n",
    "    np.cos(t_param) * 5,\n",
    "    np.sin(t_param) * 5\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Clean structure: mean={x0.mean():.2f}, std={x0.std():.2f}\")\n",
    "\n",
    "alphas = get_noise_schedule(200)\n",
    "for t in [0, 50, 100, 150, 200]:\n",
    "    x_t, _ = add_noise(x0, t, alphas)\n",
    "    print(f\"t={t:3d}: mean={x_t.mean():.2f}, std={x_t.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Single forward pass\n",
    "print(\"\\nTest 3: Diffusion Module Forward Pass\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "N_atoms = 100\n",
    "N_tokens = 32\n",
    "c_s = 384\n",
    "c_z = 128\n",
    "\n",
    "# Random noisy coordinates\n",
    "x_t = np.random.randn(N_atoms, 3) * 5\n",
    "\n",
    "# Conditioning\n",
    "single = np.random.randn(N_tokens, c_s)\n",
    "pair = np.random.randn(N_tokens, N_tokens, c_z)\n",
    "\n",
    "# Forward pass\n",
    "noise_pred = diffusion_module_forward(x_t, t=100, single=single, pair=pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Full sampling (simplified)\n",
    "print(\"\\nTest 4: Sampling (Simplified)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "N_atoms = 50\n",
    "N_tokens = 16\n",
    "\n",
    "single = np.random.randn(N_tokens, 128)\n",
    "pair = np.random.randn(N_tokens, N_tokens, 64)\n",
    "\n",
    "# Use fewer steps for speed\n",
    "final_coords, trajectory = sample_diffusion(single, pair, N_atoms, num_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification: Key Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verification: Key Properties\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Property 1: Noise schedule starts at 1, ends near 0\n",
    "alphas = get_noise_schedule(200)\n",
    "schedule_valid = alphas[0] > 0.99 and alphas[-1] < 0.01\n",
    "print(f\"Property 1 - Valid noise schedule: {schedule_valid}\")\n",
    "\n",
    "# Property 2: Adding noise increases variance\n",
    "x0 = np.random.randn(50, 3)\n",
    "x_50, _ = add_noise(x0, 50, alphas)\n",
    "x_100, _ = add_noise(x0, 100, alphas)\n",
    "variance_increases = x_50.var() <= x_100.var()\n",
    "print(f\"Property 2 - Noise increases with t: {variance_increases}\")\n",
    "\n",
    "# Property 3: Noise embedding has correct dimension\n",
    "emb = noise_level_embedding(100, c=128)\n",
    "emb_dim_correct = emb.shape == (128,)\n",
    "print(f\"Property 3 - Embedding dimension correct: {emb_dim_correct}\")\n",
    "\n",
    "# Property 4: Forward pass preserves atom count\n",
    "N_atoms = 64\n",
    "x_t = np.random.randn(N_atoms, 3)\n",
    "single = np.random.randn(16, 128)\n",
    "pair = np.random.randn(16, 16, 64)\n",
    "noise_pred = diffusion_module_forward(x_t, 50, single, pair)\n",
    "shape_preserved = noise_pred.shape == (N_atoms, 3)\n",
    "print(f\"Property 4 - Output shape preserved: {shape_preserved}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "1. **Diffusion vs Direct Prediction**: AlphaFold3 generates structures through iterative denoising rather than direct frame prediction, enabling better uncertainty quantification.\n",
    "\n",
    "2. **Noise Schedule**: The cosine schedule provides smooth noise levels, important for stable training and sampling.\n",
    "\n",
    "3. **Conditioning**: Single and pair representations condition the denoising process, incorporating sequence and evolutionary information.\n",
    "\n",
    "4. **Adaptive LayerNorm**: From DiT (Diffusion Transformer) paper, modulates features based on timestep and conditioning.\n",
    "\n",
    "5. **200 Steps**: Full inference uses 200 denoising steps, much more than AF2's 8 refinement iterations.\n",
    "\n",
    "6. **Ensemble Generation**: Multiple samples can be generated for uncertainty estimation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
