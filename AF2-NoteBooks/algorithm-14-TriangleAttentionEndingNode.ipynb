{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 14: Triangle Attention (Ending Node)\n",
    "\n",
    "Triangle attention with ending node orientation. Attends along columns where each position considers edges that share the same ending node.\n",
    "\n",
    "## Algorithm Pseudocode\n",
    "\n",
    "![TriangleAttentionEndingNode](../imgs/algorithms/TriangleAttentionEndingNode.png)\n",
    "\n",
    "## Source Code Location\n",
    "- **File**: `AF2-source-code/model/modules.py`\n",
    "- **Class**: `TriangleAttention`\n",
    "- **Lines**: 965-1045"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Starting Node (Algorithm 13)\n",
    "\n",
    "| Aspect | Starting Node (Alg 13) | Ending Node (Alg 14) |\n",
    "|--------|------------------------|----------------------|\n",
    "| Orientation | Row-wise | Column-wise |\n",
    "| Attention pattern | z[i,j] attends to z[i,k] | z[i,j] attends to z[k,j] |\n",
    "| Triangle edges | Edges starting from i | Edges ending at j |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangle_attention_ending_node(z, c=32, num_heads=4):\n",
    "    \"\"\"\n",
    "    Triangle Attention (Ending Node) - Algorithm 14.\n",
    "    \n",
    "    Attention along columns: z[i,j] attends to z[k,j] for all k.\n",
    "    \n",
    "    Args:\n",
    "        z: Pair representation [N_res, N_res, c_z]\n",
    "        c: Head dimension\n",
    "        num_heads: Number of attention heads\n",
    "    \n",
    "    Returns:\n",
    "        Updated z [N_res, N_res, c_z]\n",
    "    \"\"\"\n",
    "    N_res, _, c_z = z.shape\n",
    "    \n",
    "    # Layer normalization\n",
    "    z_norm = (z - z.mean(axis=-1, keepdims=True)) / (z.std(axis=-1, keepdims=True) + 1e-5)\n",
    "    \n",
    "    # Linear projections (random weights for demo)\n",
    "    W_q = np.random.randn(c_z, num_heads, c) * 0.02\n",
    "    W_k = np.random.randn(c_z, num_heads, c) * 0.02\n",
    "    W_v = np.random.randn(c_z, num_heads, c) * 0.02\n",
    "    W_b = np.random.randn(c_z, num_heads) * 0.02\n",
    "    W_g = np.random.randn(c_z, num_heads, c) * 0.02\n",
    "    W_o = np.random.randn(num_heads, c, c_z) * 0.02\n",
    "    \n",
    "    # Compute Q, K, V\n",
    "    q = np.einsum('ijc,chd->ijhd', z_norm, W_q)  # [N, N, H, c]\n",
    "    k = np.einsum('ijc,chd->ijhd', z_norm, W_k)\n",
    "    v = np.einsum('ijc,chd->ijhd', z_norm, W_v)\n",
    "    \n",
    "    # Bias from pair representation\n",
    "    b = np.einsum('ijc,ch->ijh', z_norm, W_b)  # [N, N, H]\n",
    "    \n",
    "    # Gating\n",
    "    g = 1 / (1 + np.exp(-np.einsum('ijc,chd->ijhd', z_norm, W_g)))  # Sigmoid\n",
    "    \n",
    "    # ENDING NODE: Transpose for column-wise attention\n",
    "    # Swap first two dimensions to attend along columns\n",
    "    q_t = q.transpose(1, 0, 2, 3)  # [N, N, H, c] -> [j, i, H, c]\n",
    "    k_t = k.transpose(1, 0, 2, 3)  # [j, k, H, c]\n",
    "    v_t = v.transpose(1, 0, 2, 3)\n",
    "    b_t = b.transpose(1, 0, 2)  # [j, k, H]\n",
    "    \n",
    "    # Attention weights along columns\n",
    "    # For each column j: attn[i,k] = softmax(q[i] @ k[k].T + b[k])\n",
    "    # q_t[j,i], k_t[j,k] -> attn[j,i,h,k]\n",
    "    attn_logits = np.einsum('jihd,jkhd->jihk', q_t, k_t) / np.sqrt(c)\n",
    "    # b_t: [j, k, H] -> expand to [j, 1, H, k]\n",
    "    attn_logits += b_t.transpose(0, 2, 1)[:, None, :, :]  # [j, 1, H, k]\n",
    "    \n",
    "    # Softmax over k dimension\n",
    "    attn_logits_max = attn_logits.max(axis=-1, keepdims=True)\n",
    "    attn_weights = np.exp(attn_logits - attn_logits_max)\n",
    "    attn_weights /= attn_weights.sum(axis=-1, keepdims=True)\n",
    "    \n",
    "    # Apply attention (column-wise)\n",
    "    attended = np.einsum('jihk,jkhd->jihd', attn_weights, v_t)\n",
    "    \n",
    "    # Transpose back to [i, j, H, c]\n",
    "    attended = attended.transpose(1, 0, 2, 3)\n",
    "    \n",
    "    # Apply gating\n",
    "    gated = g * attended\n",
    "    \n",
    "    # Output projection\n",
    "    output = np.einsum('ijhd,hdc->ijc', gated, W_o)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "N_res, c_z = 16, 64\n",
    "z = np.random.randn(N_res, N_res, c_z)\n",
    "\n",
    "print(\"Test Triangle Attention (Ending Node)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Input shape: {z.shape}\")\n",
    "\n",
    "output = triangle_attention_ending_node(z, c=32, num_heads=4)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Shape preserved: {output.shape == z.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code Reference\n",
    "\n",
    "```python\n",
    "# From AF2-source-code/model/modules.py\n",
    "\n",
    "class TriangleAttention(hk.Module):\n",
    "  \"\"\"Triangle Attention.\n",
    "\n",
    "  Jumper et al. (2021) Suppl. Alg. 13 \"TriangleAttentionStartingNode\"\n",
    "  Jumper et al. (2021) Suppl. Alg. 14 \"TriangleAttentionEndingNode\"\n",
    "  \"\"\"\n",
    "\n",
    "  def __call__(self, pair_act, pair_mask, is_training=False):\n",
    "    c = self.config\n",
    "    \n",
    "    # For ending node: transpose first\n",
    "    if c.orientation == 'per_column':\n",
    "      pair_act = jnp.swapaxes(pair_act, -2, -3)\n",
    "      pair_mask = jnp.swapaxes(pair_mask, -1, -2)\n",
    "    \n",
    "    # ... attention computation ...\n",
    "    \n",
    "    # Transpose back\n",
    "    if c.orientation == 'per_column':\n",
    "      pair_act = jnp.swapaxes(pair_act, -2, -3)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
