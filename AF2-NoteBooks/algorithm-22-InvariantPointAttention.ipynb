{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 22: Invariant Point Attention (IPA)\n",
    "\n",
    "Invariant Point Attention is a key innovation in AlphaFold2's Structure Module. It enables attention between residues that is **equivariant** to global rotations and translations - the attention patterns depend only on relative positions in 3D space, not absolute coordinates.\n",
    "\n",
    "## Algorithm Pseudocode\n",
    "\n",
    "![Invariant Point Attention](../imgs/algorithms/InvariantPointAttention.png)\n",
    "\n",
    "## Source Code Location\n",
    "- **File**: `AF2-source-code/model/folding.py`\n",
    "- **Class**: `InvariantPointAttention`\n",
    "- **Lines**: 37-279"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "### SE(3) Equivariance\n",
    "- The attention mechanism is invariant to global rotations and translations\n",
    "- Each residue has a local reference frame (rotation + translation)\n",
    "- Points are computed in local frames and transformed to global frame for distance computation\n",
    "\n",
    "### Three Types of Attention Contributions\n",
    "1. **Scalar attention**: Standard query-key dot product\n",
    "2. **Point attention**: Euclidean distance between points in 3D space\n",
    "3. **Pair bias**: Bias from pair representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rigid Body Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_matrix_from_angles(angles):\n",
    "    \"\"\"Create rotation matrix from Euler angles (simplified).\"\"\"\n",
    "    # Using quaternion-like initialization for simplicity\n",
    "    theta = np.linalg.norm(angles)\n",
    "    if theta < 1e-6:\n",
    "        return np.eye(3)\n",
    "    axis = angles / theta\n",
    "    K = np.array([[0, -axis[2], axis[1]],\n",
    "                  [axis[2], 0, -axis[0]],\n",
    "                  [-axis[1], axis[0], 0]])\n",
    "    return np.eye(3) + np.sin(theta) * K + (1 - np.cos(theta)) * K @ K\n",
    "\n",
    "\n",
    "def apply_rigid_transform(points, rotation, translation):\n",
    "    \"\"\"Apply rigid transformation: R @ points + t\"\"\"\n",
    "    return np.einsum('...ij,...j->...i', rotation, points) + translation\n",
    "\n",
    "\n",
    "def invert_rigid_transform(points, rotation, translation):\n",
    "    \"\"\"Apply inverse transformation: R^T @ (points - t)\"\"\"\n",
    "    return np.einsum('...ij,...j->...i', rotation.swapaxes(-1, -2), points - translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Implementation of IPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    \"\"\"Numerically stable softmax.\"\"\"\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "def invariant_point_attention(\n",
    "    inputs_1d,       # [N_res, c_s] single representation\n",
    "    inputs_2d,       # [N_res, N_res, c_z] pair representation\n",
    "    rotations,       # [N_res, 3, 3] rotation matrices\n",
    "    translations,    # [N_res, 3] translation vectors\n",
    "    mask,            # [N_res] residue mask\n",
    "    num_head=12,\n",
    "    num_scalar_qk=16,\n",
    "    num_scalar_v=16,\n",
    "    num_point_qk=4,\n",
    "    num_point_v=8\n",
    "):\n",
    "    \"\"\"\n",
    "    Invariant Point Attention.\n",
    "    \n",
    "    Algorithm 22 from AlphaFold2 supplementary materials.\n",
    "    \n",
    "    Args:\n",
    "        inputs_1d: Single representation [N_res, c_s]\n",
    "        inputs_2d: Pair representation [N_res, N_res, c_z]\n",
    "        rotations: Rotation matrices for each residue [N_res, 3, 3]\n",
    "        translations: Translation vectors for each residue [N_res, 3]\n",
    "        mask: Residue mask [N_res]\n",
    "    \n",
    "    Returns:\n",
    "        Updated single representation [N_res, c_s]\n",
    "    \"\"\"\n",
    "    N_res, c_s = inputs_1d.shape\n",
    "    c_z = inputs_2d.shape[-1]\n",
    "    \n",
    "    print(f\"Input shapes: 1D={inputs_1d.shape}, 2D={inputs_2d.shape}\")\n",
    "    \n",
    "    # ========== Step 1: Compute scalar queries, keys, values ==========\n",
    "    # (Lines 2-4 in algorithm)\n",
    "    q_scalar_w = np.random.randn(c_s, num_head * num_scalar_qk) * 0.01\n",
    "    k_scalar_w = np.random.randn(c_s, num_head * num_scalar_qk) * 0.01\n",
    "    v_scalar_w = np.random.randn(c_s, num_head * num_scalar_v) * 0.01\n",
    "    \n",
    "    q_scalar = (inputs_1d @ q_scalar_w).reshape(N_res, num_head, num_scalar_qk)\n",
    "    k_scalar = (inputs_1d @ k_scalar_w).reshape(N_res, num_head, num_scalar_qk)\n",
    "    v_scalar = (inputs_1d @ v_scalar_w).reshape(N_res, num_head, num_scalar_v)\n",
    "    \n",
    "    print(f\"Scalar Q/K/V shapes: {q_scalar.shape}\")\n",
    "    \n",
    "    # ========== Step 2: Compute point queries, keys, values ==========\n",
    "    # (Lines 5-7 in algorithm)\n",
    "    # Points are generated in local frame and transformed to global frame\n",
    "    \n",
    "    q_point_local_w = np.random.randn(c_s, num_head * 3 * num_point_qk) * 0.01\n",
    "    kv_point_local_w = np.random.randn(c_s, num_head * 3 * (num_point_qk + num_point_v)) * 0.01\n",
    "    \n",
    "    # Generate local points\n",
    "    q_point_local = (inputs_1d @ q_point_local_w).reshape(N_res, num_head, num_point_qk, 3)\n",
    "    kv_point_local = (inputs_1d @ kv_point_local_w).reshape(N_res, num_head, num_point_qk + num_point_v, 3)\n",
    "    k_point_local = kv_point_local[:, :, :num_point_qk, :]\n",
    "    v_point_local = kv_point_local[:, :, num_point_qk:, :]\n",
    "    \n",
    "    # Transform points to global frame\n",
    "    # q_point_global[i] = R[i] @ q_point_local[i] + t[i]\n",
    "    q_point_global = np.zeros_like(q_point_local)\n",
    "    k_point_global = np.zeros_like(k_point_local)\n",
    "    v_point_global = np.zeros_like(v_point_local)\n",
    "    \n",
    "    for i in range(N_res):\n",
    "        for h in range(num_head):\n",
    "            for p in range(num_point_qk):\n",
    "                q_point_global[i, h, p] = rotations[i] @ q_point_local[i, h, p] + translations[i]\n",
    "                k_point_global[i, h, p] = rotations[i] @ k_point_local[i, h, p] + translations[i]\n",
    "            for p in range(num_point_v):\n",
    "                v_point_global[i, h, p] = rotations[i] @ v_point_local[i, h, p] + translations[i]\n",
    "    \n",
    "    print(f\"Point Q/K shapes: {q_point_global.shape}\")\n",
    "    \n",
    "    # ========== Step 3: Compute attention logits ==========\n",
    "    # (Lines 8-12 in algorithm)\n",
    "    \n",
    "    # 3.1: Scalar attention\n",
    "    scalar_variance = max(num_scalar_qk, 1) * 1.0\n",
    "    scalar_weights = np.sqrt(1.0 / (3.0 * scalar_variance))\n",
    "    \n",
    "    # [N_res, num_head, num_scalar_qk] x [N_res, num_head, num_scalar_qk] -> [num_head, N_res, N_res]\n",
    "    attn_scalar = scalar_weights * np.einsum('qhc,khc->hqk', q_scalar, k_scalar)\n",
    "    \n",
    "    print(f\"Scalar attention shape: {attn_scalar.shape}\")\n",
    "    \n",
    "    # 3.2: Point attention (squared distance in global frame)\n",
    "    point_variance = max(num_point_qk, 1) * 9.0 / 2.0\n",
    "    point_weights = np.sqrt(1.0 / (3.0 * point_variance))\n",
    "    \n",
    "    # Trainable per-head point weights\n",
    "    trainable_point_weights = np.ones(num_head)  # Simplified, actual uses softplus\n",
    "    \n",
    "    # Compute squared distances between query and key points\n",
    "    # [num_head, N_res, N_res, num_point_qk]\n",
    "    q_point_t = q_point_global.transpose(1, 0, 2, 3)  # [num_head, N_res, num_point_qk, 3]\n",
    "    k_point_t = k_point_global.transpose(1, 0, 2, 3)\n",
    "    \n",
    "    dist2 = np.sum(\n",
    "        (q_point_t[:, :, None, :, :] - k_point_t[:, None, :, :, :]) ** 2,\n",
    "        axis=-1\n",
    "    )  # [num_head, N_res, N_res, num_point_qk]\n",
    "    \n",
    "    attn_point = -0.5 * point_weights * trainable_point_weights[:, None, None, None] * dist2\n",
    "    attn_point = np.sum(attn_point, axis=-1)  # [num_head, N_res, N_res]\n",
    "    \n",
    "    print(f\"Point attention shape: {attn_point.shape}\")\n",
    "    \n",
    "    # 3.3: Pair bias\n",
    "    attention_2d_w = np.random.randn(c_z, num_head) * 0.01\n",
    "    attention_2d = np.einsum('ijc,ch->hij', inputs_2d, attention_2d_w)\n",
    "    attention_2d *= np.sqrt(1.0 / 3.0)\n",
    "    \n",
    "    print(f\"Pair bias shape: {attention_2d.shape}\")\n",
    "    \n",
    "    # 3.4: Combine all attention terms\n",
    "    attn_logits = attn_scalar + attn_point + attention_2d\n",
    "    \n",
    "    # Apply mask\n",
    "    mask_2d = mask[:, None] * mask[None, :]  # [N_res, N_res]\n",
    "    attn_logits = attn_logits - 1e5 * (1.0 - mask_2d)\n",
    "    \n",
    "    # Softmax\n",
    "    attn = softmax(attn_logits, axis=-1)  # [num_head, N_res, N_res]\n",
    "    \n",
    "    print(f\"Attention weights shape: {attn.shape}\")\n",
    "    \n",
    "    # ========== Step 4: Compute outputs ==========\n",
    "    # (Lines 13-16 in algorithm)\n",
    "    \n",
    "    # 4.1: Scalar output\n",
    "    v_scalar_t = v_scalar.transpose(1, 0, 2)  # [num_head, N_res, num_scalar_v]\n",
    "    result_scalar = np.einsum('hqk,hkc->hqc', attn, v_scalar_t)  # [num_head, N_res, num_scalar_v]\n",
    "    result_scalar = result_scalar.transpose(1, 0, 2).reshape(N_res, num_head * num_scalar_v)\n",
    "    \n",
    "    print(f\"Scalar output shape: {result_scalar.shape}\")\n",
    "    \n",
    "    # 4.2: Point output (weighted sum of value points in global frame)\n",
    "    v_point_t = v_point_global.transpose(1, 0, 2, 3)  # [num_head, N_res, num_point_v, 3]\n",
    "    result_point_global = np.einsum('hqk,hkpc->hqpc', attn, v_point_t)\n",
    "    result_point_global = result_point_global.transpose(1, 0, 2, 3)  # [N_res, num_head, num_point_v, 3]\n",
    "    \n",
    "    # Transform back to local frame\n",
    "    result_point_local = np.zeros_like(result_point_global)\n",
    "    for i in range(N_res):\n",
    "        for h in range(num_head):\n",
    "            for p in range(num_point_v):\n",
    "                result_point_local[i, h, p] = rotations[i].T @ (result_point_global[i, h, p] - translations[i])\n",
    "    \n",
    "    result_point_local = result_point_local.reshape(N_res, num_head * num_point_v * 3)\n",
    "    \n",
    "    # Point norms (distance from origin in local frame)\n",
    "    result_point_norm = np.sqrt(\n",
    "        np.sum(result_point_local.reshape(N_res, num_head * num_point_v, 3) ** 2, axis=-1) + 1e-8\n",
    "    ).reshape(N_res, num_head * num_point_v)\n",
    "    \n",
    "    print(f\"Point output shape: {result_point_local.shape}\")\n",
    "    \n",
    "    # 4.3: Pair output\n",
    "    result_pair = np.einsum('hqk,qkc->qhc', attn, inputs_2d)\n",
    "    result_pair = result_pair.reshape(N_res, num_head * c_z)\n",
    "    \n",
    "    print(f\"Pair output shape: {result_pair.shape}\")\n",
    "    \n",
    "    # ========== Step 5: Concatenate and project ==========\n",
    "    output_features = np.concatenate([\n",
    "        result_scalar,\n",
    "        result_point_local,\n",
    "        result_point_norm,\n",
    "        result_pair\n",
    "    ], axis=-1)\n",
    "    \n",
    "    print(f\"Concatenated features shape: {output_features.shape}\")\n",
    "    \n",
    "    # Final projection\n",
    "    output_w = np.random.randn(output_features.shape[-1], c_s) * 0.01\n",
    "    output = output_features @ output_w\n",
    "    \n",
    "    print(f\"Final output shape: {output.shape}\")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parameters\n",
    "N_res = 16     # Number of residues\n",
    "c_s = 384      # Single representation dimension\n",
    "c_z = 128      # Pair representation dimension\n",
    "\n",
    "# Create test inputs\n",
    "inputs_1d = np.random.randn(N_res, c_s).astype(np.float32)\n",
    "inputs_2d = np.random.randn(N_res, N_res, c_z).astype(np.float32)\n",
    "\n",
    "# Create random rigid body transformations for each residue\n",
    "rotations = np.array([rotation_matrix_from_angles(np.random.randn(3) * 0.1) for _ in range(N_res)])\n",
    "translations = np.random.randn(N_res, 3) * 5  # Random positions in Angstroms\n",
    "\n",
    "# Mask (all valid)\n",
    "mask = np.ones(N_res, dtype=np.float32)\n",
    "\n",
    "print(f\"Number of residues: {N_res}\")\n",
    "print(f\"Single representation dim: {c_s}\")\n",
    "print(f\"Pair representation dim: {c_z}\")\n",
    "print(f\"Rotation matrices shape: {rotations.shape}\")\n",
    "print(f\"Translations shape: {translations.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run IPA\n",
    "output = invariant_point_attention(\n",
    "    inputs_1d,\n",
    "    inputs_2d,\n",
    "    rotations,\n",
    "    translations,\n",
    "    mask,\n",
    "    num_head=8,\n",
    "    num_scalar_qk=16,\n",
    "    num_scalar_v=16,\n",
    "    num_point_qk=4,\n",
    "    num_point_v=8\n",
    ")\n",
    "\n",
    "print(f\"\\nOutput statistics: mean={output.mean():.6f}, std={output.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify SE(3) Invariance\n",
    "\n",
    "Apply a global rotation and translation to all residues. The attention patterns should remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global transformation\n",
    "global_rotation = rotation_matrix_from_angles(np.array([0.5, 0.3, 0.7]))\n",
    "global_translation = np.array([10.0, 20.0, 30.0])\n",
    "\n",
    "# Apply to all residue frames\n",
    "rotations_transformed = np.array([global_rotation @ r for r in rotations])\n",
    "translations_transformed = np.array([global_rotation @ t + global_translation for t in translations])\n",
    "\n",
    "# Run IPA with transformed frames\n",
    "output_transformed = invariant_point_attention(\n",
    "    inputs_1d,\n",
    "    inputs_2d,\n",
    "    rotations_transformed,\n",
    "    translations_transformed,\n",
    "    mask,\n",
    "    num_head=8,\n",
    "    num_scalar_qk=16,\n",
    "    num_scalar_v=16,\n",
    "    num_point_qk=4,\n",
    "    num_point_v=8\n",
    ")\n",
    "\n",
    "# The outputs should be similar (not exactly equal due to point outputs being in local frames)\n",
    "print(f\"Original output mean: {output.mean():.6f}\")\n",
    "print(f\"Transformed output mean: {output_transformed.mean():.6f}\")\n",
    "print(f\"Difference norm: {np.linalg.norm(output - output_transformed):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code Reference\n",
    "\n",
    "```python\n",
    "# From AF2-source-code/model/folding.py\n",
    "\n",
    "class InvariantPointAttention(hk.Module):\n",
    "  \"\"\"Invariant Point attention module.\n",
    "\n",
    "  Jumper et al. (2021) Suppl. Alg. 22 \"InvariantPointAttention\"\n",
    "  \"\"\"\n",
    "\n",
    "  def __call__(self, inputs_1d, inputs_2d, mask, affine):\n",
    "    # Scalar queries/keys/values\n",
    "    q_scalar = Linear(num_head * num_scalar_qk)(inputs_1d)\n",
    "    k_scalar, v_scalar = split(Linear(...)(inputs_1d))\n",
    "    \n",
    "    # Point queries/keys/values in local frame\n",
    "    q_point_local = Linear(num_head * 3 * num_point_qk)(inputs_1d)\n",
    "    # Transform to global frame\n",
    "    q_point_global = affine.apply_to_point(q_point_local, extra_dims=1)\n",
    "    \n",
    "    # Compute attention from:\n",
    "    # 1. Scalar dot product\n",
    "    # 2. Point distance (invariant to global transform)\n",
    "    # 3. Pair bias\n",
    "    attn_logits = attn_scalar + attn_point + attention_2d\n",
    "    attn = softmax(attn_logits)\n",
    "    \n",
    "    # Aggregate values and project\n",
    "    result = concat([scalar_result, point_result, pair_result])\n",
    "    return output_projection(result)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "1. **SE(3) Invariance**: Point attention uses squared distances, which are invariant to global rotations/translations.\n",
    "\n",
    "2. **Three Attention Sources**: Scalar (sequence), point (3D geometry), and pair (pairwise features) attention are combined.\n",
    "\n",
    "3. **Local/Global Frames**: Points are generated in local frames but distances are computed in global frame for invariance.\n",
    "\n",
    "4. **Value Transformation**: Point values are transformed back to local frames for equivariant output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
