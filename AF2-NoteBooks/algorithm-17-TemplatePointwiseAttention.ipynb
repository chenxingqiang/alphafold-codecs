{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 17: Template Pointwise Attention\n",
    "\n",
    "Template Pointwise Attention aggregates information from multiple templates into a single pair representation using attention.\n",
    "\n",
    "## Algorithm Pseudocode\n",
    "\n",
    "![TemplatePointwiseAttention](../imgs/algorithms/TemplatePointwiseAttention.png)\n",
    "\n",
    "## Source Code Location\n",
    "- **File**: `AF2-source-code/model/modules.py`\n",
    "- **Class**: `Attention` (used for template attention)\n",
    "- **Location**: Within `EmbeddingsAndEvoformer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "When multiple templates are available, this attention mechanism:\n",
    "1. Uses the current pair representation as query\n",
    "2. Attends over template features\n",
    "3. Produces a weighted combination of template information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_pointwise_attention(z, t, num_heads=4):\n",
    "    \"\"\"\n",
    "    Template Pointwise Attention - Algorithm 17.\n",
    "    \n",
    "    Aggregates template information using attention.\n",
    "    \n",
    "    Args:\n",
    "        z: Pair representation [N_res, N_res, c_z] (query)\n",
    "        t: Template features [N_templ, N_res, N_res, c_t] (key/value)\n",
    "        num_heads: Number of attention heads\n",
    "    \n",
    "    Returns:\n",
    "        Template-aggregated features [N_res, N_res, c_z]\n",
    "    \"\"\"\n",
    "    N_res, _, c_z = z.shape\n",
    "    N_templ = t.shape[0]\n",
    "    c_t = t.shape[-1]\n",
    "    c = c_z // num_heads\n",
    "    \n",
    "    print(f\"Template Pointwise Attention\")\n",
    "    print(f\"  Query (pair): [{N_res}, {N_res}, {c_z}]\")\n",
    "    print(f\"  Key/Value (templates): [{N_templ}, {N_res}, {N_res}, {c_t}]\")\n",
    "    print(f\"  Heads: {num_heads}, Head dim: {c}\")\n",
    "    \n",
    "    # Weights\n",
    "    W_q = np.random.randn(c_z, num_heads, c) * 0.02\n",
    "    W_k = np.random.randn(c_t, num_heads, c) * 0.02\n",
    "    W_v = np.random.randn(c_t, num_heads, c) * 0.02\n",
    "    W_o = np.random.randn(num_heads, c, c_z) * 0.02\n",
    "    \n",
    "    # Query from pair representation\n",
    "    # z: [N_res, N_res, c_z] -> q: [N_res, N_res, H, c]\n",
    "    q = np.einsum('ijc,chd->ijhd', z, W_q)\n",
    "    \n",
    "    # Key/Value from templates\n",
    "    # t: [N_templ, N_res, N_res, c_t] -> k,v: [N_templ, N_res, N_res, H, c]\n",
    "    k = np.einsum('tijc,chd->tijhd', t, W_k)\n",
    "    v = np.einsum('tijc,chd->tijhd', t, W_v)\n",
    "    \n",
    "    # Attention over templates\n",
    "    # For each (i,j) position, attend over N_templ templates\n",
    "    # q[i,j]: [H, c], k[t,i,j]: [H, c] for t in templates\n",
    "    \n",
    "    # Reshape for batch attention\n",
    "    q_flat = q.reshape(N_res * N_res, num_heads, c)  # [N*N, H, c]\n",
    "    k_flat = k.reshape(N_templ, N_res * N_res, num_heads, c).transpose(1, 0, 2, 3)  # [N*N, T, H, c]\n",
    "    v_flat = v.reshape(N_templ, N_res * N_res, num_heads, c).transpose(1, 0, 2, 3)\n",
    "    \n",
    "    # Attention weights: [N*N, H, T]\n",
    "    attn_logits = np.einsum('bhc,bthc->bht', q_flat, k_flat) / np.sqrt(c)\n",
    "    attn_weights = np.exp(attn_logits - attn_logits.max(axis=-1, keepdims=True))\n",
    "    attn_weights /= attn_weights.sum(axis=-1, keepdims=True)\n",
    "    \n",
    "    # Apply attention: [N*N, H, c]\n",
    "    attended = np.einsum('bht,bthc->bhc', attn_weights, v_flat)\n",
    "    \n",
    "    # Reshape and project\n",
    "    attended = attended.reshape(N_res, N_res, num_heads, c)\n",
    "    output = np.einsum('ijhc,hcd->ijd', attended, W_o)\n",
    "    \n",
    "    print(f\"  Output: {output.shape}\")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "N_res, c_z = 32, 64\n",
    "N_templ, c_t = 4, 64\n",
    "\n",
    "z = np.random.randn(N_res, N_res, c_z)\n",
    "t = np.random.randn(N_templ, N_res, N_res, c_t)\n",
    "\n",
    "print(\"Test Template Pointwise Attention\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "output = template_pointwise_attention(z, t, num_heads=4)\n",
    "print(f\"\\nShape matches pair repr: {output.shape == z.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code Reference\n",
    "\n",
    "```python\n",
    "# Template attention in EmbeddingsAndEvoformer\n",
    "# Uses standard Attention class with:\n",
    "#   - Query: current pair representation\n",
    "#   - Key/Value: processed template features\n",
    "\n",
    "# The attention mechanism aggregates information from\n",
    "# multiple templates into the pair representation\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
