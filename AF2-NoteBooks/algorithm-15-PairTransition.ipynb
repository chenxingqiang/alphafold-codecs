{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 15: Pair Transition\n",
    "\n",
    "The Pair Transition layer is a feed-forward network applied to the pair representation. It's similar to MSA Transition (Algorithm 9) but operates on the pair representation, providing non-linear transformations after attention layers.\n",
    "\n",
    "## Algorithm Pseudocode\n",
    "\n",
    "![PairTransition](../imgs/algorithms/PairTransition.png)\n",
    "\n",
    "## Source Code Location\n",
    "- **File**: `AF2-source-code/model/modules.py`\n",
    "- **Class**: `Transition`\n",
    "- **Lines**: 1162-1195"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The Pair Transition implements a standard transformer-style feed-forward network:\n",
    "\n",
    "```\n",
    "Input z [N_res, N_res, c_z]\n",
    "    ↓\n",
    "Layer Normalization\n",
    "    ↓\n",
    "Linear (c_z → n*c_z)  [Expand]\n",
    "    ↓\n",
    "ReLU Activation\n",
    "    ↓\n",
    "Linear (n*c_z → c_z)  [Compress]\n",
    "    ↓\n",
    "Output z [N_res, N_res, c_z]\n",
    "```\n",
    "\n",
    "Where `n=4` is the expansion factor (typical transformer FFN design)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, axis=-1, eps=1e-5):\n",
    "    \"\"\"Layer normalization.\"\"\"\n",
    "    mean = np.mean(x, axis=axis, keepdims=True)\n",
    "    var = np.var(x, axis=axis, keepdims=True)\n",
    "    return (x - mean) / np.sqrt(var + eps)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation.\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def pair_transition(z, n=4):\n",
    "    \"\"\"\n",
    "    Pair Transition Layer - Algorithm 15.\n",
    "    \n",
    "    Feed-forward network: LayerNorm -> Linear(expand) -> ReLU -> Linear(compress)\n",
    "    \n",
    "    Args:\n",
    "        z: Pair representation [N_res, N_res, c_z]\n",
    "        n: Expansion factor (default: 4)\n",
    "    \n",
    "    Returns:\n",
    "        Updated z [N_res, N_res, c_z]\n",
    "    \"\"\"\n",
    "    N_res, _, c_z = z.shape\n",
    "    c_hidden = n * c_z\n",
    "    \n",
    "    print(f\"Pair Transition\")\n",
    "    print(f\"=\"*50)\n",
    "    print(f\"Input: [{N_res}, {N_res}, {c_z}]\")\n",
    "    print(f\"Hidden: [{N_res}, {N_res}, {c_hidden}]\")\n",
    "    print(f\"Expansion factor: {n}x\")\n",
    "    \n",
    "    # Step 1: Layer normalization (Line 1)\n",
    "    z_norm = layer_norm(z, axis=-1)\n",
    "    print(f\"\\nStep 1 - Layer norm: {z_norm.shape}\")\n",
    "    \n",
    "    # Step 2: First linear (expand) (Line 2)\n",
    "    W1 = np.random.randn(c_z, c_hidden) * (c_z ** -0.5)\n",
    "    b1 = np.zeros(c_hidden)\n",
    "    a = np.einsum('ijc,cd->ijd', z_norm, W1) + b1\n",
    "    print(f\"Step 2 - Expand: {a.shape}\")\n",
    "    \n",
    "    # Step 3: ReLU activation (Line 3)\n",
    "    a = relu(a)\n",
    "    print(f\"Step 3 - ReLU: {a.shape}\")\n",
    "    \n",
    "    # Step 4: Second linear (compress) (Line 4)\n",
    "    W2 = np.random.randn(c_hidden, c_z) * (c_hidden ** -0.5)\n",
    "    b2 = np.zeros(c_z)\n",
    "    output = np.einsum('ijd,dc->ijc', a, W2) + b2\n",
    "    print(f\"Step 4 - Compress: {output.shape}\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "class PairTransition:\n",
    "    \"\"\"\n",
    "    Object-oriented Pair Transition for reusable weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, c_z, n=4):\n",
    "        \"\"\"Initialize weights.\"\"\"\n",
    "        self.c_z = c_z\n",
    "        self.c_hidden = n * c_z\n",
    "        self.n = n\n",
    "        \n",
    "        # Layer norm parameters\n",
    "        self.ln_scale = np.ones(c_z)\n",
    "        self.ln_bias = np.zeros(c_z)\n",
    "        \n",
    "        # Linear 1 (expand)\n",
    "        self.W1 = np.random.randn(c_z, self.c_hidden) * (c_z ** -0.5)\n",
    "        self.b1 = np.zeros(self.c_hidden)\n",
    "        \n",
    "        # Linear 2 (compress)\n",
    "        self.W2 = np.random.randn(self.c_hidden, c_z) * (self.c_hidden ** -0.5)\n",
    "        self.b2 = np.zeros(c_z)\n",
    "    \n",
    "    def __call__(self, z):\n",
    "        \"\"\"Apply pair transition.\"\"\"\n",
    "        # Layer norm\n",
    "        z_norm = layer_norm(z, axis=-1)\n",
    "        z_norm = z_norm * self.ln_scale + self.ln_bias\n",
    "        \n",
    "        # Expand -> ReLU -> Compress\n",
    "        a = np.einsum('ijc,cd->ijd', z_norm, self.W1) + self.b1\n",
    "        a = relu(a)\n",
    "        output = np.einsum('ijd,dc->ijc', a, self.W2) + self.b2\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count total parameters.\"\"\"\n",
    "        params = {\n",
    "            'layer_norm': 2 * self.c_z,\n",
    "            'linear_1': self.c_z * self.c_hidden + self.c_hidden,\n",
    "            'linear_2': self.c_hidden * self.c_z + self.c_z,\n",
    "        }\n",
    "        params['total'] = sum(params.values())\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Basic functionality\n",
    "print(\"Test 1: Basic Functionality\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "N_res, c_z = 32, 128\n",
    "z = np.random.randn(N_res, N_res, c_z).astype(np.float32)\n",
    "\n",
    "output = pair_transition(z, n=4)\n",
    "\n",
    "print(f\"\\nInput shape: {z.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Shape preserved: {output.shape == z.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Object-oriented version\n",
    "print(\"\\nTest 2: Object-Oriented Version\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "N_res, c_z = 32, 128\n",
    "z = np.random.randn(N_res, N_res, c_z).astype(np.float32)\n",
    "\n",
    "transition = PairTransition(c_z=c_z, n=4)\n",
    "\n",
    "output = transition(z)\n",
    "\n",
    "print(f\"Input shape: {z.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "params = transition.count_parameters()\n",
    "print(f\"\\nParameter counts:\")\n",
    "for name, count in params.items():\n",
    "    print(f\"  {name}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Different expansion factors\n",
    "print(\"\\nTest 3: Different Expansion Factors\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "N_res, c_z = 16, 64\n",
    "z = np.random.randn(N_res, N_res, c_z).astype(np.float32)\n",
    "\n",
    "for n in [1, 2, 4, 8]:\n",
    "    np.random.seed(42)\n",
    "    output = pair_transition(z.copy(), n=n)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Verify ReLU effect\n",
    "print(\"\\nTest 4: Verify ReLU Effect\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "N_res, c_z = 16, 64\n",
    "z = np.random.randn(N_res, N_res, c_z).astype(np.float32)\n",
    "\n",
    "# Trace through manually to check ReLU sparsity\n",
    "z_norm = layer_norm(z, axis=-1)\n",
    "c_hidden = 4 * c_z\n",
    "W1 = np.random.randn(c_z, c_hidden) * (c_z ** -0.5)\n",
    "b1 = np.zeros(c_hidden)\n",
    "a = np.einsum('ijc,cd->ijd', z_norm, W1) + b1\n",
    "\n",
    "# Check pre-ReLU distribution\n",
    "print(f\"Pre-ReLU statistics:\")\n",
    "print(f\"  Mean: {a.mean():.4f}\")\n",
    "print(f\"  Std: {a.std():.4f}\")\n",
    "print(f\"  Negative fraction: {(a < 0).mean():.2%}\")\n",
    "\n",
    "a_relu = relu(a)\n",
    "print(f\"\\nPost-ReLU statistics:\")\n",
    "print(f\"  Mean: {a_relu.mean():.4f}\")\n",
    "print(f\"  Zero fraction: {(a_relu == 0).mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: Residual connection pattern\n",
    "print(\"\\nTest 5: Residual Connection Pattern\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "N_res, c_z = 16, 64\n",
    "z = np.random.randn(N_res, N_res, c_z).astype(np.float32)\n",
    "\n",
    "# In Evoformer, transition is used with residual connection:\n",
    "# z = z + Transition(z)\n",
    "transition = PairTransition(c_z=c_z, n=4)\n",
    "delta = transition(z)\n",
    "z_updated = z + delta\n",
    "\n",
    "print(f\"Input mean: {z.mean():.4f}, std: {z.std():.4f}\")\n",
    "print(f\"Delta mean: {delta.mean():.4f}, std: {delta.std():.4f}\")\n",
    "print(f\"Output mean: {z_updated.mean():.4f}, std: {z_updated.std():.4f}\")\n",
    "print(f\"\\nResidual ratio (delta/input std): {delta.std() / z.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification: Key Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verification: Key Properties\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "N_res, c_z = 24, 128\n",
    "z = np.random.randn(N_res, N_res, c_z).astype(np.float32)\n",
    "\n",
    "output = pair_transition(z, n=4)\n",
    "\n",
    "# Property 1: Shape preserved\n",
    "shape_preserved = output.shape == z.shape\n",
    "print(f\"Property 1 - Shape preserved: {shape_preserved}\")\n",
    "\n",
    "# Property 2: Finite output\n",
    "output_finite = np.isfinite(output).all()\n",
    "print(f\"Property 2 - Output finite: {output_finite}\")\n",
    "\n",
    "# Property 3: Non-trivial transformation\n",
    "not_identity = not np.allclose(output, z)\n",
    "print(f\"Property 3 - Non-trivial: {not_identity}\")\n",
    "\n",
    "# Property 4: Element-wise operation (no cross-position mixing)\n",
    "# Change one position, only that position should change\n",
    "z_mod = z.copy()\n",
    "z_mod[0, 0, :] += 1.0\n",
    "output_mod = pair_transition(z_mod, n=4)\n",
    "\n",
    "# Due to layer norm, all positions are affected slightly\n",
    "# But the most affected should be [0, 0]\n",
    "diff = np.abs(output_mod - output)\n",
    "max_diff_pos = np.unravel_index(np.argmax(diff.sum(axis=-1)), diff.shape[:2])\n",
    "print(f\"Property 4 - Most affected position: {max_diff_pos} (expected: (0, 0))\")\n",
    "\n",
    "# Property 5: Reasonable output scale\n",
    "output_scale = output.std()\n",
    "input_scale = z.std()\n",
    "scale_ratio = output_scale / input_scale\n",
    "print(f\"Property 5 - Scale ratio: {scale_ratio:.4f} (should be ~1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code Reference\n",
    "\n",
    "```python\n",
    "# From AF2-source-code/model/modules.py\n",
    "\n",
    "class Transition(hk.Module):\n",
    "  \"\"\"Transition layer.\n",
    "\n",
    "  Jumper et al. (2021) Suppl. Alg. 9 \"MSATransition\"\n",
    "  Jumper et al. (2021) Suppl. Alg. 15 \"PairTransition\"\n",
    "  \n",
    "  This class is used for both MSA and Pair transitions.\n",
    "  \"\"\"\n",
    "\n",
    "  def __call__(self, act, mask, is_training=False):\n",
    "    c = self.config\n",
    "    num_intermediate = int(act.shape[-1] * c.num_intermediate_factor)\n",
    "    \n",
    "    # Layer norm\n",
    "    act = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True,\n",
    "                       name='input_layer_norm')(act)\n",
    "    \n",
    "    # Expand\n",
    "    act = common_modules.Linear(\n",
    "        num_intermediate,\n",
    "        initializer='relu',\n",
    "        name='transition1')(act)\n",
    "    \n",
    "    # ReLU\n",
    "    act = jax.nn.relu(act)\n",
    "    \n",
    "    # Compress\n",
    "    act = common_modules.Linear(\n",
    "        c.num_channel,\n",
    "        initializer=utils.final_init(self.global_config),\n",
    "        name='transition2')(act)\n",
    "    \n",
    "    return act\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "1. **Shared Architecture**: The same `Transition` class is used for both MSA (Algorithm 9) and Pair (Algorithm 15) transitions.\n",
    "\n",
    "2. **Expansion Factor**: The default expansion factor of 4x is a common transformer design choice, allowing more expressive non-linear transformations.\n",
    "\n",
    "3. **Position-Independent**: Unlike attention layers, the transition operates independently on each position (after layer norm).\n",
    "\n",
    "4. **ReLU Sparsity**: ReLU typically zeros out ~50% of the hidden activations, creating sparse intermediate representations.\n",
    "\n",
    "5. **Residual Connection**: In practice, the transition output is added to the input (residual connection), not used directly.\n",
    "\n",
    "6. **Parameter Cost**: The transition layers contribute significantly to parameter count due to the 4x expansion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
