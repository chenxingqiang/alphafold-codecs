{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 9: MSA Transition\n",
    "\n",
    "The MSA Transition is a simple feed-forward network (two-layer MLP) applied to each position independently. It provides additional non-linear transformation capacity between attention layers.\n",
    "\n",
    "## Algorithm Pseudocode\n",
    "\n",
    "![MSA Transition](../imgs/algorithms/MSATransition.png)\n",
    "\n",
    "## Source Code Location\n",
    "- **File**: `AF2-source-code/model/modules.py`\n",
    "- **Class**: `Transition`\n",
    "- **Lines**: 476-529\n",
    "\n",
    "**Note**: The same `Transition` class is used for both MSA Transition (Alg 9) and Pair Transition (Alg 15)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "The transition block follows a standard pattern:\n",
    "1. **Layer Normalization**: Normalize input\n",
    "2. **Linear Expansion**: Project to 4x wider hidden dimension\n",
    "3. **ReLU Activation**: Non-linearity\n",
    "4. **Linear Projection**: Project back to original dimension\n",
    "5. **Residual Connection**: Add to input (applied in wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, axis=-1, eps=1e-5, gamma=None, beta=None):\n",
    "    \"\"\"Layer normalization with optional scale and offset.\"\"\"\n",
    "    mean = np.mean(x, axis=axis, keepdims=True)\n",
    "    var = np.var(x, axis=axis, keepdims=True)\n",
    "    normalized = (x - mean) / np.sqrt(var + eps)\n",
    "    \n",
    "    if gamma is not None:\n",
    "        normalized = normalized * gamma\n",
    "    if beta is not None:\n",
    "        normalized = normalized + beta\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "\n",
    "def transition(act, mask, num_intermediate_factor=4):\n",
    "    \"\"\"\n",
    "    MSA Transition / Pair Transition.\n",
    "    \n",
    "    Algorithm 9 (MSA Transition) and Algorithm 15 (Pair Transition)\n",
    "    from AlphaFold2 supplementary materials.\n",
    "    \n",
    "    A simple 2-layer MLP with expansion and contraction.\n",
    "    \n",
    "    Args:\n",
    "        act: Input activations [..., N, c]\n",
    "        mask: Mask [..., N]\n",
    "        num_intermediate_factor: Expansion factor (default 4)\n",
    "    \n",
    "    Returns:\n",
    "        Output activations [..., N, c] (same shape as input)\n",
    "    \"\"\"\n",
    "    *batch_dims, n, c = act.shape\n",
    "    num_intermediate = int(c * num_intermediate_factor)\n",
    "    \n",
    "    print(f\"Input shape: {act.shape}\")\n",
    "    print(f\"Channel dimension: {c}\")\n",
    "    print(f\"Intermediate dimension: {num_intermediate}\")\n",
    "    \n",
    "    # Expand mask for broadcasting\n",
    "    mask_expanded = mask[..., None]\n",
    "    \n",
    "    # Step 1: Layer normalization (Line 1)\n",
    "    gamma = np.ones(c)  # Learnable scale\n",
    "    beta = np.zeros(c)  # Learnable offset\n",
    "    act_norm = layer_norm(act, axis=-1, gamma=gamma, beta=beta)\n",
    "    \n",
    "    print(f\"After LayerNorm: {act_norm.shape}\")\n",
    "    \n",
    "    # Step 2: First linear layer - expansion (Line 2)\n",
    "    # Expand from c to 4*c\n",
    "    w1 = np.random.randn(c, num_intermediate) * np.sqrt(2.0 / c)  # He init for ReLU\n",
    "    b1 = np.zeros(num_intermediate)\n",
    "    \n",
    "    hidden = np.einsum('...c,cd->...d', act_norm, w1) + b1\n",
    "    \n",
    "    print(f\"After expansion: {hidden.shape}\")\n",
    "    \n",
    "    # Step 3: ReLU activation (Line 3)\n",
    "    hidden = relu(hidden)\n",
    "    \n",
    "    # Step 4: Second linear layer - contraction (Line 4)\n",
    "    # Contract from 4*c back to c\n",
    "    w2 = np.random.randn(num_intermediate, c) * 0.01  # Small init for residual\n",
    "    b2 = np.zeros(c)\n",
    "    \n",
    "    output = np.einsum('...d,dc->...c', hidden, w2) + b2\n",
    "    \n",
    "    print(f\"After contraction: {output.shape}\")\n",
    "    \n",
    "    # Note: Residual connection is applied in the wrapper function (dropout_wrapper)\n",
    "    # output = input + dropout(transition(input))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: MSA Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parameters for MSA\n",
    "N_seq = 128    # Number of sequences\n",
    "N_res = 64     # Number of residues\n",
    "c_m = 256      # MSA channel dimension\n",
    "\n",
    "# Create test inputs\n",
    "msa_act = np.random.randn(N_seq, N_res, c_m).astype(np.float32)\n",
    "msa_mask = np.ones((N_seq, N_res), dtype=np.float32)\n",
    "\n",
    "print(\"MSA Transition Test\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Input: {msa_act.shape}\")\n",
    "print()\n",
    "\n",
    "# Run transition\n",
    "msa_update = transition(msa_act, msa_mask, num_intermediate_factor=4)\n",
    "\n",
    "print(f\"\\nOutput statistics:\")\n",
    "print(f\"  Mean: {msa_update.mean():.6f}\")\n",
    "print(f\"  Std: {msa_update.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Pair Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parameters for Pair representation\n",
    "N_res = 64     # Number of residues\n",
    "c_z = 128      # Pair channel dimension\n",
    "\n",
    "# Create test inputs\n",
    "pair_act = np.random.randn(N_res, N_res, c_z).astype(np.float32)\n",
    "pair_mask = np.ones((N_res, N_res), dtype=np.float32)\n",
    "\n",
    "print(\"\\nPair Transition Test\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Input: {pair_act.shape}\")\n",
    "print()\n",
    "\n",
    "# Run transition\n",
    "pair_update = transition(pair_act, pair_mask, num_intermediate_factor=4)\n",
    "\n",
    "print(f\"\\nOutput statistics:\")\n",
    "print(f\"  Mean: {pair_update.mean():.6f}\")\n",
    "print(f\"  Std: {pair_update.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Transition with Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_with_residual(act, mask, dropout_rate=0.0, num_intermediate_factor=4):\n",
    "    \"\"\"\n",
    "    Complete transition block including residual connection.\n",
    "    \n",
    "    This matches how it's called in the Evoformer via dropout_wrapper.\n",
    "    \"\"\"\n",
    "    # Compute transition\n",
    "    update = transition(act, mask, num_intermediate_factor)\n",
    "    \n",
    "    # Apply dropout (skip in inference)\n",
    "    if dropout_rate > 0:\n",
    "        mask_drop = np.random.binomial(1, 1 - dropout_rate, update.shape)\n",
    "        update = update * mask_drop / (1 - dropout_rate)\n",
    "    \n",
    "    # Residual connection\n",
    "    output = act + update\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# Test with residual\n",
    "print(\"\\nTransition with Residual Connection\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Before\n",
    "input_norm = np.linalg.norm(msa_act)\n",
    "print(f\"Input norm: {input_norm:.4f}\")\n",
    "\n",
    "# Apply transition\n",
    "output = transition_with_residual(msa_act, msa_mask)\n",
    "\n",
    "# After\n",
    "output_norm = np.linalg.norm(output)\n",
    "update_norm = np.linalg.norm(output - msa_act)\n",
    "\n",
    "print(f\"Output norm: {output_norm:.4f}\")\n",
    "print(f\"Update norm: {update_norm:.4f}\")\n",
    "print(f\"Update relative to input: {update_norm / input_norm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code Reference\n",
    "\n",
    "```python\n",
    "# From AF2-source-code/model/modules.py\n",
    "\n",
    "class Transition(hk.Module):\n",
    "  \"\"\"Transition layer.\n",
    "\n",
    "  Jumper et al. (2021) Suppl. Alg. 9 \"MSATransition\"\n",
    "  Jumper et al. (2021) Suppl. Alg. 15 \"PairTransition\"\n",
    "  \"\"\"\n",
    "\n",
    "  def __call__(self, act, mask, is_training=True):\n",
    "    _, _, nc = act.shape\n",
    "\n",
    "    num_intermediate = int(nc * self.config.num_intermediate_factor)\n",
    "    mask = jnp.expand_dims(mask, axis=-1)\n",
    "\n",
    "    act = hk.LayerNorm(\n",
    "        axis=[-1],\n",
    "        create_scale=True,\n",
    "        create_offset=True,\n",
    "        name='input_layer_norm')(act)\n",
    "\n",
    "    transition_module = hk.Sequential([\n",
    "        common_modules.Linear(\n",
    "            num_intermediate,\n",
    "            initializer='relu',\n",
    "            name='transition1'), \n",
    "        jax.nn.relu,\n",
    "        common_modules.Linear(\n",
    "            nc,\n",
    "            initializer=utils.final_init(self.global_config),\n",
    "            name='transition2')\n",
    "    ])\n",
    "\n",
    "    act = mapping.inference_subbatch(\n",
    "        transition_module,\n",
    "        self.global_config.subbatch_size,\n",
    "        batched_args=[act],\n",
    "        nonbatched_args=[],\n",
    "        low_memory=not is_training)\n",
    "\n",
    "    return act\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "1. **Expansion Factor**: Default is 4x, meaning hidden dimension is 4 times the input dimension (256 â†’ 1024 for MSA).\n",
    "\n",
    "2. **Position-wise**: Applied independently to each position (same as Transformer FFN).\n",
    "\n",
    "3. **Weight Initialization**: \n",
    "   - First layer uses 'relu' init (He initialization)\n",
    "   - Second layer uses small initialization for stable residual learning\n",
    "\n",
    "4. **Shared Implementation**: Same class handles both MSA (Algorithm 9) and Pair (Algorithm 15) transitions.\n",
    "\n",
    "5. **Subbatching**: For memory efficiency, the transition can be applied in chunks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
