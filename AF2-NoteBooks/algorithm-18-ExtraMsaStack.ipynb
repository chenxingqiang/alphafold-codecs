{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 18: Extra MSA Stack\n",
    "\n",
    "The Extra MSA Stack processes additional MSA sequences that don't fit in the main MSA. It updates the pair representation without maintaining a full MSA representation, using global attention for efficiency.\n",
    "\n",
    "## Algorithm Pseudocode\n",
    "\n",
    "![ExtraMsaStack](../imgs/algorithms/ExtraMsaStack.png)\n",
    "\n",
    "## Source Code Location\n",
    "- **File**: `AF2-source-code/model/modules.py`\n",
    "- **Class**: `ExtraMsaStack`, `ExtraMsaStackIteration`\n",
    "- **Lines**: 1483-1560"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### Comparison: Main MSA vs Extra MSA\n",
    "\n",
    "| Aspect | Main Evoformer | Extra MSA Stack |\n",
    "|--------|----------------|------------------|\n",
    "| MSA size | ~512 sequences | ~5000 sequences |\n",
    "| Attention | Row + Column | Row + Global Column |\n",
    "| Output | MSA + Pair | Pair only |\n",
    "| Blocks | 48 | 4 |\n",
    "| Purpose | Main processing | Extract pair info |\n",
    "\n",
    "### Algorithm Steps\n",
    "\n",
    "1. MSA Row Attention with Pair Bias\n",
    "2. MSA Column Global Attention (Algorithm 19)\n",
    "3. MSA Transition\n",
    "4. Outer Product Mean → Pair update\n",
    "5. Triangle Multiplication + Attention\n",
    "6. Pair Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, axis=-1, eps=1e-5):\n",
    "    \"\"\"Layer normalization.\"\"\"\n",
    "    mean = np.mean(x, axis=axis, keepdims=True)\n",
    "    var = np.var(x, axis=axis, keepdims=True)\n",
    "    return (x - mean) / np.sqrt(var + eps)\n",
    "\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    \"\"\"Softmax.\"\"\"\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU.\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid.\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-np.clip(x, -20, 20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msa_row_attention(m, z, c=8, num_heads=8):\n",
    "    \"\"\"MSA Row Attention with Pair Bias (simplified).\"\"\"\n",
    "    N_seq, N_res, c_m = m.shape\n",
    "    c_z = z.shape[-1]\n",
    "    \n",
    "    m_norm = layer_norm(m)\n",
    "    z_norm = layer_norm(z)\n",
    "    \n",
    "    W_q = np.random.randn(c_m, num_heads, c) * (c_m ** -0.5)\n",
    "    W_k = np.random.randn(c_m, num_heads, c) * (c_m ** -0.5)\n",
    "    W_v = np.random.randn(c_m, num_heads, c) * (c_m ** -0.5)\n",
    "    W_b = np.random.randn(c_z, num_heads) * (c_z ** -0.5)\n",
    "    W_o = np.random.randn(num_heads, c, c_m) * ((num_heads * c) ** -0.5)\n",
    "    \n",
    "    q = np.einsum('src,chd->srhd', m_norm, W_q)\n",
    "    k = np.einsum('src,chd->srhd', m_norm, W_k)\n",
    "    v = np.einsum('src,chd->srhd', m_norm, W_v)\n",
    "    b = np.einsum('ijc,ch->ijh', z_norm, W_b)\n",
    "    \n",
    "    attn_logits = np.einsum('sihd,sjhd->sijh', q, k) / np.sqrt(c)\n",
    "    attn_logits = attn_logits + b[None, :, :, :]  # Add pair bias\n",
    "    attn_weights = softmax(attn_logits, axis=2)\n",
    "    \n",
    "    attended = np.einsum('sijh,sjhd->sihd', attn_weights, v)\n",
    "    output = np.einsum('sihd,hdc->sic', attended, W_o)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def msa_column_global_attention(m, c=8, num_heads=8):\n",
    "    \"\"\"MSA Column Global Attention (Algorithm 19).\"\"\"\n",
    "    N_seq, N_res, c_m = m.shape\n",
    "    \n",
    "    m_norm = layer_norm(m)\n",
    "    \n",
    "    W_q = np.random.randn(c_m, num_heads, c) * (c_m ** -0.5)\n",
    "    W_k = np.random.randn(c_m, num_heads, c) * (c_m ** -0.5)\n",
    "    W_v = np.random.randn(c_m, num_heads, c) * (c_m ** -0.5)\n",
    "    W_g = np.random.randn(c_m, num_heads, c) * (c_m ** -0.5)\n",
    "    W_o = np.random.randn(num_heads, c, c_m) * ((num_heads * c) ** -0.5)\n",
    "    \n",
    "    # Global query from mean\n",
    "    q_input = m_norm.mean(axis=0)  # [N_res, c_m]\n",
    "    q = np.einsum('rc,chd->rhd', q_input, W_q)  # [N_res, H, c]\n",
    "    \n",
    "    k = np.einsum('src,chd->srhd', m_norm, W_k)\n",
    "    v = np.einsum('src,chd->srhd', m_norm, W_v)\n",
    "    g = sigmoid(np.einsum('src,chd->srhd', m_norm, W_g))\n",
    "    \n",
    "    # Attention over sequences\n",
    "    attn_logits = np.einsum('rhd,srhd->rhs', q, k) / np.sqrt(c)\n",
    "    attn_weights = softmax(attn_logits, axis=-1)\n",
    "    \n",
    "    attended = np.einsum('rhs,srhd->rhd', attn_weights, v)\n",
    "    attended_broadcast = np.broadcast_to(attended[None, :, :, :], (N_seq, N_res, num_heads, c))\n",
    "    \n",
    "    gated = g * attended_broadcast\n",
    "    output = np.einsum('srhd,hdc->src', gated, W_o)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def outer_product_mean(m, c=32, c_z=128):\n",
    "    \"\"\"Outer Product Mean (Algorithm 10).\"\"\"\n",
    "    N_seq, N_res, c_m = m.shape\n",
    "    \n",
    "    m_norm = layer_norm(m)\n",
    "    \n",
    "    W_l = np.random.randn(c_m, c) * (c_m ** -0.5)\n",
    "    W_r = np.random.randn(c_m, c) * (c_m ** -0.5)\n",
    "    W_o = np.random.randn(c * c, c_z) * ((c * c) ** -0.5)\n",
    "    \n",
    "    left = np.einsum('src,cd->srd', m_norm, W_l)\n",
    "    right = np.einsum('src,cd->srd', m_norm, W_r)\n",
    "    \n",
    "    outer = np.einsum('sic,sjd->sijcd', left, right)\n",
    "    outer_mean = outer.mean(axis=0)  # [N, N, c, c]\n",
    "    outer_flat = outer_mean.reshape(N_res, N_res, c * c)\n",
    "    \n",
    "    output = np.einsum('ijc,cd->ijd', outer_flat, W_o)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_msa_stack(m_extra, z, num_blocks=4):\n",
    "    \"\"\"\n",
    "    Extra MSA Stack - Algorithm 18.\n",
    "    \n",
    "    Processes large extra MSA to update pair representation.\n",
    "    \n",
    "    Args:\n",
    "        m_extra: Extra MSA representation [N_extra, N_res, c_m]\n",
    "        z: Pair representation [N_res, N_res, c_z]\n",
    "        num_blocks: Number of stack iterations\n",
    "    \n",
    "    Returns:\n",
    "        Updated pair representation z\n",
    "    \"\"\"\n",
    "    N_extra, N_res, c_m = m_extra.shape\n",
    "    c_z = z.shape[-1]\n",
    "    \n",
    "    print(f\"Extra MSA Stack\")\n",
    "    print(f\"=\"*50)\n",
    "    print(f\"Extra MSA: [{N_extra}, {N_res}, {c_m}]\")\n",
    "    print(f\"Pair: [{N_res}, {N_res}, {c_z}]\")\n",
    "    print(f\"Blocks: {num_blocks}\")\n",
    "    \n",
    "    for block_idx in range(num_blocks):\n",
    "        print(f\"\\nBlock {block_idx + 1}:\")\n",
    "        \n",
    "        # Step 1: MSA Row Attention with Pair Bias\n",
    "        m_extra = m_extra + msa_row_attention(m_extra, z, c=8, num_heads=8)\n",
    "        print(f\"  MSA Row Attention\")\n",
    "        \n",
    "        # Step 2: MSA Column Global Attention (Algorithm 19)\n",
    "        m_extra = m_extra + msa_column_global_attention(m_extra, c=8, num_heads=8)\n",
    "        print(f\"  MSA Column Global Attention\")\n",
    "        \n",
    "        # Step 3: MSA Transition\n",
    "        m_norm = layer_norm(m_extra)\n",
    "        W1 = np.random.randn(c_m, 4 * c_m) * (c_m ** -0.5)\n",
    "        W2 = np.random.randn(4 * c_m, c_m) * ((4 * c_m) ** -0.5)\n",
    "        m_extra = m_extra + relu(m_norm @ W1) @ W2\n",
    "        print(f\"  MSA Transition\")\n",
    "        \n",
    "        # Step 4: Outer Product Mean -> Update Pair\n",
    "        z = z + outer_product_mean(m_extra, c=32, c_z=c_z)\n",
    "        print(f\"  Outer Product Mean -> Pair\")\n",
    "        \n",
    "        # Step 5-8: Triangle operations on pair (simplified)\n",
    "        z_norm = layer_norm(z)\n",
    "        z = z + z_norm * 0.1  # Simplified triangle ops\n",
    "        print(f\"  Triangle Operations\")\n",
    "        \n",
    "        print(f\"  MSA norm: {np.linalg.norm(m_extra):.2f}, Pair norm: {np.linalg.norm(z):.2f}\")\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Basic functionality\n",
    "print(\"Test 1: Basic Functionality\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "N_extra, N_res, c_m = 1024, 32, 64\n",
    "c_z = 128\n",
    "\n",
    "m_extra = np.random.randn(N_extra, N_res, c_m).astype(np.float32)\n",
    "z = np.random.randn(N_res, N_res, c_z).astype(np.float32)\n",
    "\n",
    "z_updated = extra_msa_stack(m_extra, z.copy(), num_blocks=2)\n",
    "\n",
    "print(f\"\\nOutput pair shape: {z_updated.shape}\")\n",
    "print(f\"Pair shape preserved: {z_updated.shape == z.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Global attention efficiency\n",
    "print(\"\\nTest 2: Global Attention Efficiency\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import time\n",
    "\n",
    "N_res, c_m = 32, 64\n",
    "\n",
    "for N_seq in [256, 512, 1024, 2048]:\n",
    "    m = np.random.randn(N_seq, N_res, c_m).astype(np.float32)\n",
    "    \n",
    "    start = time.time()\n",
    "    _ = msa_column_global_attention(m, c=8, num_heads=8)\n",
    "    elapsed = (time.time() - start) * 1000\n",
    "    \n",
    "    print(f\"N_seq={N_seq:4d}: {elapsed:.2f}ms (O(N) complexity)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Information flow from MSA to pair\n",
    "print(\"\\nTest 3: Information Flow MSA -> Pair\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "N_extra, N_res, c_m = 512, 24, 32\n",
    "c_z = 64\n",
    "\n",
    "# Create structured MSA\n",
    "m_extra = np.random.randn(N_extra, N_res, c_m).astype(np.float32)\n",
    "\n",
    "# Start with zero pair\n",
    "z_zero = np.zeros((N_res, N_res, c_z), dtype=np.float32)\n",
    "\n",
    "# Run stack\n",
    "z_result = extra_msa_stack(m_extra, z_zero, num_blocks=2)\n",
    "\n",
    "print(f\"\\nStarting pair norm: {np.linalg.norm(z_zero):.4f}\")\n",
    "print(f\"Final pair norm: {np.linalg.norm(z_result):.4f}\")\n",
    "print(f\"Information transferred from MSA to pair\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification: Key Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verification: Key Properties\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "N_extra, N_res, c_m = 256, 20, 32\n",
    "c_z = 64\n",
    "\n",
    "m_extra = np.random.randn(N_extra, N_res, c_m).astype(np.float32)\n",
    "z = np.random.randn(N_res, N_res, c_z).astype(np.float32)\n",
    "\n",
    "z_output = extra_msa_stack(m_extra.copy(), z.copy(), num_blocks=2)\n",
    "\n",
    "# Property 1: Pair shape preserved\n",
    "shape_preserved = z_output.shape == z.shape\n",
    "print(f\"Property 1 - Pair shape preserved: {shape_preserved}\")\n",
    "\n",
    "# Property 2: Finite output\n",
    "output_finite = np.isfinite(z_output).all()\n",
    "print(f\"Property 2 - Output finite: {output_finite}\")\n",
    "\n",
    "# Property 3: Non-trivial update\n",
    "not_identity = not np.allclose(z_output, z)\n",
    "print(f\"Property 3 - Non-trivial update: {not_identity}\")\n",
    "\n",
    "# Property 4: Depends on extra MSA\n",
    "m_modified = m_extra * 2.0\n",
    "z_modified = extra_msa_stack(m_modified, z.copy(), num_blocks=2)\n",
    "depends_on_msa = not np.allclose(z_output, z_modified)\n",
    "print(f\"Property 4 - Depends on extra MSA: {depends_on_msa}\")\n",
    "\n",
    "# Property 5: Reasonable scale\n",
    "scale_ratio = np.linalg.norm(z_output) / np.linalg.norm(z)\n",
    "print(f\"Property 5 - Scale ratio: {scale_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code Reference\n",
    "\n",
    "```python\n",
    "# From AF2-source-code/model/modules.py\n",
    "\n",
    "class ExtraMsaStack(hk.Module):\n",
    "  \"\"\"Extra MSA Stack.\n",
    "\n",
    "  Jumper et al. (2021) Suppl. Alg. 18 \"ExtraMsaStack\"\n",
    "  \"\"\"\n",
    "\n",
    "  def __call__(self, msa_act, pair_act, msa_mask, pair_mask, is_training):\n",
    "    for _ in range(self.config.num_block):\n",
    "      pair_act, msa_act = ExtraMsaStackIteration(\n",
    "          self.config, self.global_config)(msa_act, pair_act, ...)\n",
    "    return pair_act\n",
    "\n",
    "\n",
    "class ExtraMsaStackIteration(hk.Module):\n",
    "  \"\"\"Single iteration of Extra MSA Stack.\n",
    "  \n",
    "  Key difference: Uses GlobalAttention instead of regular column attention.\n",
    "  \"\"\"\n",
    "  \n",
    "  def __call__(self, msa_act, pair_act, ...):\n",
    "    # MSA row attention with pair bias\n",
    "    msa_act += MSARowAttentionWithPairBias(...)(msa_act, msa_mask, pair_act)\n",
    "    \n",
    "    # MSA column global attention (Algorithm 19)\n",
    "    msa_act += MSAColumnGlobalAttention(...)(msa_act, msa_mask)\n",
    "    \n",
    "    # MSA transition\n",
    "    msa_act += Transition(...)(msa_act, msa_mask)\n",
    "    \n",
    "    # Outer product mean -> pair update\n",
    "    pair_act += OuterProductMean(...)(msa_act, msa_mask)\n",
    "    \n",
    "    # Triangle operations on pair\n",
    "    pair_act += TriangleMultiplication(...)(pair_act, pair_mask)\n",
    "    pair_act += TriangleAttention(...)(pair_act, pair_mask)\n",
    "    pair_act += Transition(...)(pair_act, pair_mask)\n",
    "    \n",
    "    return pair_act, msa_act\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "1. **Handling Large MSAs**: Deep MSAs can have 10,000+ sequences. The Extra MSA Stack processes them efficiently.\n",
    "\n",
    "2. **Global Attention**: O(N) complexity per position instead of O(N²), enabling processing of large MSAs.\n",
    "\n",
    "3. **Pair-Only Output**: Unlike the main Evoformer, only the pair representation is updated and passed on.\n",
    "\n",
    "4. **Fewer Blocks**: 4 blocks vs 48 for Evoformer - the extra MSA provides supplementary information.\n",
    "\n",
    "5. **Co-evolution Signal**: The outer product mean extracts pairwise co-evolution signals from the extra MSA.\n",
    "\n",
    "6. **Two-Track Architecture**: The split into main MSA (detailed processing) and extra MSA (efficient processing) is a key architectural innovation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
