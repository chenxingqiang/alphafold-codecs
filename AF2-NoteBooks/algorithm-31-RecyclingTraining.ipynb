{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 31: Recycling (Training)\n",
    "\n",
    "During training, recycling is handled differently to enable gradient computation while maintaining memory efficiency.\n",
    "\n",
    "## Algorithm Pseudocode\n",
    "\n",
    "![RecyclingTraining](../imgs/algorithms/RecyclingTraining.png)\n",
    "\n",
    "## Source Code Location\n",
    "- **File**: `AF2-source-code/model/modules.py`\n",
    "- **Class**: `AlphaFold`\n",
    "- **Lines**: 123-200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training vs Inference Recycling\n",
    "\n",
    "| Aspect | Inference | Training |\n",
    "|--------|-----------|----------|\n",
    "| Recycles | Fixed (3) | Random (0-3) |\n",
    "| Gradients | Not needed | Only on final |\n",
    "| stop_gradient | On all | On prev tensors |\n",
    "| Memory | Full | Checkpoint |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recycling_training(batch, model_fn, max_recycle=3):\n",
    "    \"\"\"\n",
    "    Recycling Training - Algorithm 31.\n",
    "    \n",
    "    Training-time recycling with random number of iterations.\n",
    "    \n",
    "    Args:\n",
    "        batch: Input features\n",
    "        model_fn: Forward pass function\n",
    "        max_recycle: Maximum recycling iterations\n",
    "    \n",
    "    Returns:\n",
    "        Final prediction (with gradients only on final iteration)\n",
    "    \"\"\"\n",
    "    N_res = batch['aatype'].shape[0]\n",
    "    \n",
    "    # Sample random number of recycles\n",
    "    num_recycle = np.random.randint(0, max_recycle + 1)\n",
    "    \n",
    "    print(f\"Recycling Training\")\n",
    "    print(f\"  Residues: {N_res}\")\n",
    "    print(f\"  Sampled recycles: {num_recycle} (max: {max_recycle})\")\n",
    "    \n",
    "    # Initialize recycled tensors\n",
    "    prev = {\n",
    "        'prev_pos': np.zeros((N_res, 37, 3)),\n",
    "        'prev_msa_first_row': np.zeros((N_res, 256)),\n",
    "        'prev_pair': np.zeros((N_res, N_res, 128)),\n",
    "    }\n",
    "    \n",
    "    # Recycling loop (no gradients until final)\n",
    "    for i in range(num_recycle):\n",
    "        print(f\"\\n  Iteration {i} (no gradient):\")\n",
    "        \n",
    "        # Forward pass WITHOUT gradients\n",
    "        # In JAX: jax.lax.stop_gradient\n",
    "        output = model_fn(batch, prev, is_training=True)\n",
    "        \n",
    "        # Update prev with stop_gradient\n",
    "        prev['prev_pos'] = output['final_atom_positions'].copy()  # stop_gradient\n",
    "        prev['prev_msa_first_row'] = output['msa_first_row'].copy()\n",
    "        prev['prev_pair'] = output['pair_repr'].copy()\n",
    "        \n",
    "        print(f\"    stop_gradient applied to prev tensors\")\n",
    "    \n",
    "    # Final iteration WITH gradients\n",
    "    print(f\"\\n  Final iteration (with gradient):\")\n",
    "    final_output = model_fn(batch, prev, is_training=True)\n",
    "    print(f\"    Computing loss and gradients\")\n",
    "    \n",
    "    return final_output\n",
    "\n",
    "\n",
    "def dummy_model(batch, prev, is_training=True):\n",
    "    \"\"\"Dummy model function for testing.\"\"\"\n",
    "    N_res = batch['aatype'].shape[0]\n",
    "    \n",
    "    return {\n",
    "        'final_atom_positions': prev['prev_pos'] + np.random.randn(N_res, 37, 3),\n",
    "        'msa_first_row': prev['prev_msa_first_row'] + np.random.randn(N_res, 256) * 0.1,\n",
    "        'pair_repr': prev['prev_pair'] + np.random.randn(N_res, N_res, 128) * 0.1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "batch = {\n",
    "    'aatype': np.random.randint(0, 20, size=64),\n",
    "}\n",
    "\n",
    "print(\"Test Recycling Training\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "output = recycling_training(batch, dummy_model, max_recycle=3)\n",
    "\n",
    "print(f\"\\nFinal output keys: {list(output.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple runs to show random sampling\n",
    "print(\"\\nRandom recycle sampling:\")\n",
    "for _ in range(5):\n",
    "    num_recycle = np.random.randint(0, 4)\n",
    "    print(f\"  Run: {num_recycle} recycles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code Reference\n",
    "\n",
    "```python\n",
    "# From AF2-source-code/model/modules.py\n",
    "\n",
    "class AlphaFold(hk.Module):\n",
    "  \"\"\"AlphaFold model with recycling.\n",
    "\n",
    "  Jumper et al. (2021) Suppl. Alg. 31 \"RecyclingTraining\"\n",
    "  \"\"\"\n",
    "\n",
    "  def __call__(self, batch, is_training, ...):\n",
    "    if is_training:\n",
    "      # Random number of recycles during training\n",
    "      num_iter = hk.get_state('num_recycles', ...)\n",
    "      \n",
    "      # Run iterations without gradients\n",
    "      for i in range(num_iter):\n",
    "        ret = impl(batch_i, jax.lax.stop_gradient(prev), ...)\n",
    "        prev = get_prev(ret)\n",
    "      \n",
    "      # Final iteration with gradients\n",
    "      ret = impl(batch, jax.lax.stop_gradient(prev), is_training=True, ...)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
