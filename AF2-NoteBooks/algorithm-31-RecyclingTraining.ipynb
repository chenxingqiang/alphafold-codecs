{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 31: Recycling (Training)\n",
    "\n",
    "During training, recycling is handled differently to enable gradient computation while maintaining memory efficiency. Gradients only flow through the final iteration, with previous iterations treated as fixed.\n",
    "\n",
    "## Algorithm Pseudocode\n",
    "\n",
    "![RecyclingTraining](../imgs/algorithms/RecyclingTraining.png)\n",
    "\n",
    "## Source Code Location\n",
    "- **File**: `AF2-source-code/model/modules.py`\n",
    "- **Class**: `AlphaFold`\n",
    "- **Lines**: 123-200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### Training vs Inference Recycling\n",
    "\n",
    "| Aspect | Inference (Alg 30) | Training (Alg 31) |\n",
    "|--------|-------------------|-------------------|\n",
    "| Recycles | Fixed (3) | Random (0 to max) |\n",
    "| Gradients | None needed | Only on final iteration |\n",
    "| stop_gradient | On all prev | On all prev |\n",
    "| Memory | Moderate | Optimized |\n",
    "\n",
    "### Random Recycling\n",
    "\n",
    "During training:\n",
    "1. Sample a random number of recycles (0 to max_recycle)\n",
    "2. Run that many iterations without gradients\n",
    "3. Run final iteration WITH gradients\n",
    "4. Compute loss and backpropagate only through final iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_gradient(x):\n",
    "    \"\"\"\n",
    "    Simulated stop_gradient operation.\n",
    "    \n",
    "    In JAX: jax.lax.stop_gradient(x)\n",
    "    In PyTorch: x.detach()\n",
    "    \n",
    "    Here we just return a copy (NumPy has no gradients).\n",
    "    \"\"\"\n",
    "    if isinstance(x, dict):\n",
    "        return {k: stop_gradient(v) for k, v in x.items()}\n",
    "    elif isinstance(x, np.ndarray):\n",
    "        return x.copy()\n",
    "    elif x is None:\n",
    "        return None\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel:\n",
    "    \"\"\"Simplified model for demonstrating training recycling.\"\"\"\n",
    "    \n",
    "    def __init__(self, c_m=256, c_z=128):\n",
    "        self.c_m = c_m\n",
    "        self.c_z = c_z\n",
    "    \n",
    "    def __call__(self, batch, prev, is_training=True):\n",
    "        N_res = batch['aatype'].shape[0]\n",
    "        \n",
    "        # Simulate forward pass\n",
    "        msa_repr = np.random.randn(1, N_res, self.c_m) * 0.1\n",
    "        pair_repr = np.random.randn(N_res, N_res, self.c_z) * 0.1\n",
    "        \n",
    "        if prev['prev_msa_first_row'] is not None:\n",
    "            msa_repr[0] = msa_repr[0] + prev['prev_msa_first_row']\n",
    "        \n",
    "        if prev['prev_pair'] is not None:\n",
    "            pair_repr = pair_repr + prev['prev_pair'] * 0.1\n",
    "        \n",
    "        atom_positions = np.random.randn(N_res, 37, 3) * 5\n",
    "        if prev['prev_pos'] is not None:\n",
    "            atom_positions = prev['prev_pos'] + np.random.randn(N_res, 37, 3) * 0.5\n",
    "        \n",
    "        return {\n",
    "            'final_atom_positions': atom_positions,\n",
    "            'msa_first_row': msa_repr[0],\n",
    "            'pair_repr': pair_repr,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recycling_training(batch, model, max_recycle=3):\n",
    "    \"\"\"\n",
    "    Recycling Training - Algorithm 31.\n",
    "    \n",
    "    Training-time recycling with random number of iterations.\n",
    "    \n",
    "    Args:\n",
    "        batch: Input features including ground truth\n",
    "        model: Model to train\n",
    "        max_recycle: Maximum recycling iterations\n",
    "    \n",
    "    Returns:\n",
    "        Final prediction and loss info\n",
    "    \"\"\"\n",
    "    N_res = batch['aatype'].shape[0]\n",
    "    \n",
    "    # STEP 1: Sample random number of recycles (Algorithm 31, Line 1)\n",
    "    num_recycle = np.random.randint(0, max_recycle + 1)\n",
    "    \n",
    "    print(f\"Recycling Training\")\n",
    "    print(f\"=\"*50)\n",
    "    print(f\"Residues: {N_res}\")\n",
    "    print(f\"Sampled recycles: {num_recycle} (max: {max_recycle})\")\n",
    "    \n",
    "    # Initialize previous outputs\n",
    "    prev = {\n",
    "        'prev_pos': None,\n",
    "        'prev_msa_first_row': None,\n",
    "        'prev_pair': None,\n",
    "    }\n",
    "    \n",
    "    # STEP 2: Recycling loop WITHOUT gradients (Lines 2-6)\n",
    "    for i in range(num_recycle):\n",
    "        print(f\"\\nIteration {i} (no gradient):\")\n",
    "        \n",
    "        # stop_gradient on previous outputs (Line 3)\n",
    "        prev_stopped = stop_gradient(prev)\n",
    "        \n",
    "        # Forward pass without gradients\n",
    "        output = model(batch, prev_stopped, is_training=True)\n",
    "        \n",
    "        # Update prev with stop_gradient (Line 5)\n",
    "        prev = {\n",
    "            'prev_pos': stop_gradient(output['final_atom_positions']),\n",
    "            'prev_msa_first_row': stop_gradient(output['msa_first_row']),\n",
    "            'prev_pair': stop_gradient(output['pair_repr']),\n",
    "        }\n",
    "        \n",
    "        print(f\"  stop_gradient applied to prev tensors\")\n",
    "    \n",
    "    # STEP 3: Final iteration WITH gradients (Lines 7-9)\n",
    "    print(f\"\\nFinal iteration (WITH gradient):\")\n",
    "    \n",
    "    # stop_gradient on prev, but not on model outputs\n",
    "    prev_stopped = stop_gradient(prev)\n",
    "    \n",
    "    # This is where gradients would flow in real training\n",
    "    final_output = model(batch, prev_stopped, is_training=True)\n",
    "    \n",
    "    print(f\"  Gradients flow through this iteration\")\n",
    "    \n",
    "    # STEP 4: Compute loss (would use ground truth)\n",
    "    if 'gt_positions' in batch:\n",
    "        pos_error = np.mean((final_output['final_atom_positions'] - batch['gt_positions']) ** 2)\n",
    "        print(f\"  Position MSE: {pos_error:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'output': final_output,\n",
    "        'num_recycle': num_recycle,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Basic functionality\n",
    "print(\"Test 1: Basic Functionality\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "batch = {\n",
    "    'aatype': np.random.randint(0, 20, size=64),\n",
    "    'gt_positions': np.random.randn(64, 37, 3) * 10,\n",
    "}\n",
    "\n",
    "model = SimpleModel(c_m=256, c_z=128)\n",
    "result = recycling_training(batch, model, max_recycle=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Random recycle sampling distribution\n",
    "print(\"\\nTest 2: Recycle Sampling Distribution\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "max_recycle = 3\n",
    "n_samples = 10000\n",
    "\n",
    "recycle_counts = np.zeros(max_recycle + 1)\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    num_recycle = np.random.randint(0, max_recycle + 1)\n",
    "    recycle_counts[num_recycle] += 1\n",
    "\n",
    "print(f\"Distribution of recycles over {n_samples} samples:\")\n",
    "for i in range(max_recycle + 1):\n",
    "    pct = recycle_counts[i] / n_samples * 100\n",
    "    print(f\"  {i} recycles: {pct:.1f}%\")\n",
    "\n",
    "print(f\"\\nExpected: uniform {100/(max_recycle+1):.1f}% each\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Multiple training steps with different recycles\n",
    "print(\"\\nTest 3: Multiple Training Steps\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "batch = {\n",
    "    'aatype': np.random.randint(0, 20, size=32),\n",
    "    'gt_positions': np.random.randn(32, 37, 3) * 10,\n",
    "}\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "for step in range(5):\n",
    "    print(f\"\\n--- Training step {step + 1} ---\")\n",
    "    result = recycling_training(batch, model, max_recycle=3)\n",
    "    print(f\"Used {result['num_recycle']} recycles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Zero recycles case\n",
    "print(\"\\nTest 4: Zero Recycles Case\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Force zero recycles by setting seed\n",
    "np.random.seed(0)  # This gives num_recycle=0 for uniform(0,4)\n",
    "\n",
    "batch = {\n",
    "    'aatype': np.random.randint(0, 20, size=32),\n",
    "}\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# Run multiple times until we get 0 recycles\n",
    "found_zero = False\n",
    "for _ in range(10):\n",
    "    num_recycle = np.random.randint(0, 4)\n",
    "    if num_recycle == 0:\n",
    "        found_zero = True\n",
    "        print(\"Got 0 recycles - only final iteration runs with gradients\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nWith 0 recycles:\")\n",
    "print(f\"  - No previous outputs\")\n",
    "print(f\"  - Model sees only raw inputs\")\n",
    "print(f\"  - Equivalent to non-recycling baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification: Key Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verification: Key Properties\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "batch = {\n",
    "    'aatype': np.random.randint(0, 20, size=32),\n",
    "    'gt_positions': np.random.randn(32, 37, 3) * 10,\n",
    "}\n",
    "\n",
    "model = SimpleModel()\n",
    "max_recycle = 3\n",
    "\n",
    "# Property 1: num_recycle in valid range\n",
    "valid_range = True\n",
    "for _ in range(100):\n",
    "    n = np.random.randint(0, max_recycle + 1)\n",
    "    if n < 0 or n > max_recycle:\n",
    "        valid_range = False\n",
    "        break\n",
    "print(f\"Property 1 - Recycles in [0, {max_recycle}]: {valid_range}\")\n",
    "\n",
    "# Property 2: Final output exists regardless of num_recycle\n",
    "result = recycling_training(batch, model, max_recycle=3)\n",
    "has_output = result['output'] is not None\n",
    "print(f\"Property 2 - Final output exists: {has_output}\")\n",
    "\n",
    "# Property 3: Output shape correct\n",
    "N_res = batch['aatype'].shape[0]\n",
    "shape_correct = result['output']['final_atom_positions'].shape == (N_res, 37, 3)\n",
    "print(f\"Property 3 - Output shape correct: {shape_correct}\")\n",
    "\n",
    "# Property 4: stop_gradient behavior (all prev are copies)\n",
    "# In our simulation, stop_gradient returns copies\n",
    "prev = {'x': np.array([1, 2, 3])}\n",
    "prev_stopped = stop_gradient(prev)\n",
    "prev_stopped['x'][0] = 999  # Modify copy\n",
    "unchanged = prev['x'][0] == 1  # Original unchanged\n",
    "print(f\"Property 4 - stop_gradient creates copy: {unchanged}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code Reference\n",
    "\n",
    "```python\n",
    "# From AF2-source-code/model/modules.py\n",
    "\n",
    "class AlphaFold(hk.Module):\n",
    "  \"\"\"AlphaFold model with recycling.\n",
    "\n",
    "  Jumper et al. (2021) Suppl. Alg. 31 \"RecyclingTraining\"\n",
    "  \"\"\"\n",
    "\n",
    "  def __call__(self, batch, is_training, ...):\n",
    "    if is_training:\n",
    "      # Sample random number of recycles\n",
    "      num_recycle = hk.get_state(\n",
    "          'num_recycles',\n",
    "          init=lambda: jax.random.randint(..., 0, max_recycle + 1))\n",
    "      \n",
    "      # Recycling iterations without gradients\n",
    "      for i in range(num_recycle):\n",
    "        # Apply stop_gradient to break gradient flow\n",
    "        prev = jax.lax.stop_gradient(prev)\n",
    "        ret = impl(batch, prev, is_training=True, ...)\n",
    "        prev = get_prev(ret)\n",
    "      \n",
    "      # Final iteration WITH gradients\n",
    "      prev = jax.lax.stop_gradient(prev)  # Still stop grad on prev\n",
    "      ret = impl(batch, prev, is_training=True, ...)  # Gradients flow here\n",
    "      \n",
    "      return ret\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "1. **Random Recycles**: Using a random number of recycles during training provides data augmentation and prevents the model from overfitting to a specific number of iterations.\n",
    "\n",
    "2. **stop_gradient**: The `stop_gradient` operation is crucial - it prevents gradients from flowing through previous iterations, reducing memory usage by not storing intermediate activations.\n",
    "\n",
    "3. **Only Final Gradients**: Only the final iteration contributes to gradient computation. This is memory-efficient but means the model learns to make good final predictions given (possibly imperfect) previous outputs.\n",
    "\n",
    "4. **Training Efficiency**: With random recycles, some training steps use 0 recycles (no recycling overhead), while others use up to max_recycle iterations.\n",
    "\n",
    "5. **Generalization**: Training with varying numbers of recycles helps the model generalize to inference where a fixed number is used.\n",
    "\n",
    "6. **Memory Savings**: By not computing gradients through all iterations, memory usage is roughly 1/N of what full backprop would require."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
