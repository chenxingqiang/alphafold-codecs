{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 7: MSA Row-wise Gated Self-Attention with Pair Bias\n",
    "\n",
    "This algorithm implements row-wise self-attention on the MSA representation, with an additional bias term derived from the pair representation. This is a key mechanism for incorporating structural information into the MSA processing.\n",
    "\n",
    "## Algorithm Pseudocode\n",
    "\n",
    "![MSA Row Attention with Pair Bias](../imgs/algorithms/MSARowAttentionWithPairBias.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "### Input/Output Shapes\n",
    "- **Input MSA**: `m` with shape `[N_seq, N_res, c_m]`\n",
    "- **Input Pair**: `z` with shape `[N_res, N_res, c_z]`\n",
    "- **Output**: Updated MSA with same shape `[N_seq, N_res, c_m]`\n",
    "\n",
    "### Core Ideas\n",
    "1. **Row-wise Attention**: Each sequence in the MSA attends to all residue positions within that sequence\n",
    "2. **Pair Bias**: The pair representation provides a learned bias to the attention logits, encoding structural relationships\n",
    "3. **Gating**: Output is gated by a sigmoid-activated linear projection for controlled information flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code Implementation\n",
    "\n",
    "From `AF2-source-code/model/modules.py`:\n",
    "\n",
    "```python\n",
    "class MSARowAttentionWithPairBias(hk.Module):\n",
    "  \"\"\"MSA per-row attention biased by the pair representation.\n",
    "\n",
    "  Jumper et al. (2021) Suppl. Alg. 7 \"MSARowAttentionWithPairBias\"\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, config, global_config,\n",
    "               name='msa_row_attention_with_pair_bias'):\n",
    "    super().__init__(name=name)\n",
    "    self.config = config\n",
    "    self.global_config = global_config\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass Implementation\n",
    "\n",
    "```python\n",
    "def __call__(self,\n",
    "             msa_act,\n",
    "             msa_mask,\n",
    "             pair_act,\n",
    "             is_training=False):\n",
    "    \"\"\"Builds MSARowAttentionWithPairBias module.\n",
    "\n",
    "    Arguments:\n",
    "      msa_act: [N_seq, N_res, c_m] MSA representation.\n",
    "      msa_mask: [N_seq, N_res] mask of non-padded regions.\n",
    "      pair_act: [N_res, N_res, c_z] pair representation.\n",
    "      is_training: Whether the module is in training mode.\n",
    "\n",
    "    Returns:\n",
    "      Update to msa_act, shape [N_seq, N_res, c_m].\n",
    "    \"\"\"\n",
    "    c = self.config\n",
    "\n",
    "    assert len(msa_act.shape) == 3\n",
    "    assert len(msa_mask.shape) == 2\n",
    "    assert c.orientation == 'per_row'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Attention Mask (Line 2)\n",
    "\n",
    "```python\n",
    "    # Create bias from mask: large negative value for masked positions\n",
    "    bias = (1e9 * (msa_mask - 1.))[:, None, None, :]\n",
    "    assert len(bias.shape) == 4\n",
    "```\n",
    "\n",
    "The mask is converted to a bias term:\n",
    "- Valid positions (mask=1) → bias = 0\n",
    "- Padded positions (mask=0) → bias = -1e9 (effectively -∞ after softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Layer Normalization (Line 1)\n",
    "\n",
    "```python\n",
    "    # Normalize MSA input\n",
    "    msa_act = hk.LayerNorm(\n",
    "        axis=[-1], create_scale=True, create_offset=True, name='query_norm')(\n",
    "            msa_act)\n",
    "\n",
    "    # Normalize pair representation for bias computation\n",
    "    pair_act = hk.LayerNorm(\n",
    "        axis=[-1],\n",
    "        create_scale=True,\n",
    "        create_offset=True,\n",
    "        name='feat_2d_norm')(\n",
    "            pair_act)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Compute Pair Bias (Line 3)\n",
    "\n",
    "```python\n",
    "    # Project pair representation to attention heads\n",
    "    init_factor = 1. / jnp.sqrt(int(pair_act.shape[-1]))\n",
    "    weights = hk.get_parameter(\n",
    "        'feat_2d_weights',\n",
    "        shape=(pair_act.shape[-1], c.num_head),\n",
    "        init=hk.initializers.RandomNormal(stddev=init_factor))\n",
    "    \n",
    "    # Compute non-batched bias: [N_res, N_res] -> [num_head, N_res, N_res]\n",
    "    nonbatched_bias = jnp.einsum('qkc,ch->hqk', pair_act, weights)\n",
    "```\n",
    "\n",
    "This projects the pair representation `[N_res, N_res, c_z]` to `[num_head, N_res, N_res]`, creating a bias for each attention head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Apply Attention (Lines 4-7)\n",
    "\n",
    "```python\n",
    "    # Create attention module\n",
    "    attn_mod = Attention(\n",
    "        c, self.global_config, msa_act.shape[-1])\n",
    "    \n",
    "    # Apply attention with subbatching for memory efficiency\n",
    "    msa_act = mapping.inference_subbatch(\n",
    "        attn_mod,\n",
    "        self.global_config.subbatch_size,\n",
    "        batched_args=[msa_act, msa_act, bias],\n",
    "        nonbatched_args=[nonbatched_bias],\n",
    "        low_memory=not is_training)\n",
    "\n",
    "    return msa_act\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Attention Module\n",
    "\n",
    "The underlying `Attention` class implements standard multi-head attention with gating:\n",
    "\n",
    "```python\n",
    "class Attention(hk.Module):\n",
    "  \"\"\"Multihead attention.\"\"\"\n",
    "\n",
    "  def __call__(self, q_data, m_data, bias, nonbatched_bias=None):\n",
    "    # Get dimensions\n",
    "    key_dim = self.config.get('key_dim', int(q_data.shape[-1]))\n",
    "    value_dim = self.config.get('value_dim', int(m_data.shape[-1]))\n",
    "    num_head = self.config.num_head\n",
    "    key_dim = key_dim // num_head\n",
    "    value_dim = value_dim // num_head\n",
    "\n",
    "    # Compute Q, K, V projections\n",
    "    q = jnp.einsum('bqa,ahc->bqhc', q_data, q_weights) * key_dim**(-0.5)\n",
    "    k = jnp.einsum('bka,ahc->bkhc', m_data, k_weights)\n",
    "    v = jnp.einsum('bka,ahc->bkhc', m_data, v_weights)\n",
    "    \n",
    "    # Compute attention logits with bias\n",
    "    logits = jnp.einsum('bqhc,bkhc->bhqk', q, k) + bias\n",
    "    if nonbatched_bias is not None:\n",
    "      logits += jnp.expand_dims(nonbatched_bias, axis=0)\n",
    "    \n",
    "    # Softmax and weighted sum\n",
    "    weights = jax.nn.softmax(logits)\n",
    "    weighted_avg = jnp.einsum('bhqk,bkhc->bqhc', weights, v)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gating Mechanism (Lines 8-10)\n",
    "\n",
    "```python\n",
    "    if self.config.gating:\n",
    "      # Compute gate values from query data\n",
    "      gating_weights = hk.get_parameter(\n",
    "          'gating_w',\n",
    "          shape=(q_data.shape[-1], num_head, value_dim),\n",
    "          init=hk.initializers.Constant(0.0))\n",
    "      gating_bias = hk.get_parameter(\n",
    "          'gating_b',\n",
    "          shape=(num_head, value_dim),\n",
    "          init=hk.initializers.Constant(1.0))\n",
    "\n",
    "      gate_values = jnp.einsum('bqc, chv->bqhv', q_data,\n",
    "                               gating_weights) + gating_bias\n",
    "      gate_values = jax.nn.sigmoid(gate_values)\n",
    "\n",
    "      # Apply gating\n",
    "      weighted_avg *= gate_values\n",
    "```\n",
    "\n",
    "The gating mechanism:\n",
    "- Initialized with bias=1, so initial gate ≈ sigmoid(1) ≈ 0.73\n",
    "- Allows the network to learn to suppress or amplify attention outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Projection (Line 11)\n",
    "\n",
    "```python\n",
    "    # Project back to output dimension\n",
    "    o_weights = hk.get_parameter(\n",
    "        'output_w', shape=(num_head, value_dim, self.output_dim),\n",
    "        init=init)\n",
    "    o_bias = hk.get_parameter('output_b', shape=(self.output_dim,),\n",
    "                              init=hk.initializers.Constant(0.0))\n",
    "\n",
    "    output = jnp.einsum('bqhc,hco->bqo', weighted_avg, o_weights) + o_bias\n",
    "    return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Formulation\n",
    "\n",
    "The attention computation can be written as:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + B_{\\text{pair}} + B_{\\text{mask}}\\right) V$$\n",
    "\n",
    "Where:\n",
    "- $Q, K, V$: Query, Key, Value projections from MSA\n",
    "- $d_k$: Key dimension per head\n",
    "- $B_{\\text{pair}}$: Bias from pair representation\n",
    "- $B_{\\text{mask}}$: Mask bias (-∞ for padded positions)\n",
    "\n",
    "The pair bias encodes:\n",
    "- Residue-residue distance information\n",
    "- Evolutionary coupling signals\n",
    "- Structural constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "MSA Row Attention with Pair Bias serves as a bridge between:\n",
    "- **Sequence information** (MSA representation)\n",
    "- **Structural information** (pair representation)\n",
    "\n",
    "Key features:\n",
    "1. Row-wise attention allows each sequence to refine its residue representations\n",
    "2. Pair bias injects structural knowledge into the attention pattern\n",
    "3. Gating provides learnable control over information flow\n",
    "4. Subbatching enables memory-efficient processing of long sequences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
