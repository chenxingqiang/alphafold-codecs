{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 19: MSA Column Global Attention\n",
    "\n",
    "MSA Column Global Attention is an efficient attention mechanism for processing large MSAs. Instead of full O(N²) attention, it uses a global query that summarizes all sequences.\n",
    "\n",
    "## Algorithm Pseudocode\n",
    "\n",
    "![MSAColumnGlobalAttention](../imgs/algorithms/MSAColumnGlobalAttention.png)\n",
    "\n",
    "## Source Code Location\n",
    "- **File**: `AF2-source-code/model/modules.py`\n",
    "- **Class**: `GlobalAttention`\n",
    "- **Lines**: 717-795"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Regular Column Attention\n",
    "\n",
    "| Aspect | Regular Column (Alg 8) | Global Column (Alg 19) |\n",
    "|--------|------------------------|------------------------|\n",
    "| Complexity | O(N_seq²) | O(N_seq) |\n",
    "| Query | All sequences | Mean of sequences |\n",
    "| Use case | Main MSA (~512) | Extra MSA (~5000) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msa_column_global_attention(m, num_heads=8, c=8):\n",
    "    \"\"\"\n",
    "    MSA Column Global Attention - Algorithm 19.\n",
    "    \n",
    "    Efficient attention using mean query.\n",
    "    \n",
    "    Args:\n",
    "        m: MSA representation [N_seq, N_res, c_m]\n",
    "        num_heads: Number of attention heads\n",
    "        c: Head dimension\n",
    "    \n",
    "    Returns:\n",
    "        Updated m [N_seq, N_res, c_m]\n",
    "    \"\"\"\n",
    "    N_seq, N_res, c_m = m.shape\n",
    "    \n",
    "    print(f\"MSA Column Global Attention\")\n",
    "    print(f\"  Input: [{N_seq}, {N_res}, {c_m}]\")\n",
    "    print(f\"  Heads: {num_heads}, Head dim: {c}\")\n",
    "    \n",
    "    # Layer norm\n",
    "    m_norm = (m - m.mean(axis=-1, keepdims=True)) / (m.std(axis=-1, keepdims=True) + 1e-5)\n",
    "    \n",
    "    # Weights\n",
    "    W_q = np.random.randn(c_m, num_heads, c) * 0.02\n",
    "    W_k = np.random.randn(c_m, num_heads, c) * 0.02\n",
    "    W_v = np.random.randn(c_m, num_heads, c) * 0.02\n",
    "    W_g = np.random.randn(c_m, num_heads, c) * 0.02\n",
    "    W_o = np.random.randn(num_heads, c, c_m) * 0.02\n",
    "    \n",
    "    # KEY DIFFERENCE: Global query from mean\n",
    "    # Instead of q for each sequence, compute mean query\n",
    "    q_input = m_norm.mean(axis=0)  # [N_res, c_m] - mean over sequences\n",
    "    q = np.einsum('rc,chd->rhd', q_input, W_q)  # [N_res, H, c]\n",
    "    print(f\"  Global query: [{N_res}, {num_heads}, {c}]\")\n",
    "    \n",
    "    # K, V from all sequences\n",
    "    k = np.einsum('src,chd->srhd', m_norm, W_k)  # [N_seq, N_res, H, c]\n",
    "    v = np.einsum('src,chd->srhd', m_norm, W_v)\n",
    "    \n",
    "    # Gating (per sequence)\n",
    "    g = 1 / (1 + np.exp(-np.einsum('src,chd->srhd', m_norm, W_g)))\n",
    "    \n",
    "    # Attention: global query attends to all keys\n",
    "    # For each residue position, attention over sequences\n",
    "    # q: [N_res, H, c], k: [N_seq, N_res, H, c]\n",
    "    attn_logits = np.einsum('rhd,srhd->rhs', q, k) / np.sqrt(c)  # [N_res, H, N_seq]\n",
    "    \n",
    "    # Softmax over sequences\n",
    "    attn_logits_max = attn_logits.max(axis=-1, keepdims=True)\n",
    "    attn_weights = np.exp(attn_logits - attn_logits_max)\n",
    "    attn_weights /= attn_weights.sum(axis=-1, keepdims=True)\n",
    "    \n",
    "    # Apply attention\n",
    "    # weighted sum of values: [N_res, H, c]\n",
    "    attended = np.einsum('rhs,srhd->rhd', attn_weights, v)\n",
    "    \n",
    "    # Broadcast to all sequences and apply gating\n",
    "    # attended: [N_res, H, c] -> broadcast to [N_seq, N_res, H, c]\n",
    "    attended_broadcast = np.broadcast_to(attended[None, :, :, :], (N_seq, N_res, num_heads, c))\n",
    "    gated = g * attended_broadcast\n",
    "    \n",
    "    # Output projection\n",
    "    output = np.einsum('srhd,hdc->src', gated, W_o)\n",
    "    \n",
    "    print(f\"  Output: {output.shape}\")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "N_seq, N_res, c_m = 1024, 32, 64  # Large MSA\n",
    "\n",
    "m = np.random.randn(N_seq, N_res, c_m)\n",
    "\n",
    "print(\"Test MSA Column Global Attention\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "output = msa_column_global_attention(m, num_heads=8, c=8)\n",
    "print(f\"\\nShape preserved: {output.shape == m.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code Reference\n",
    "\n",
    "```python\n",
    "# From AF2-source-code/model/modules.py\n",
    "\n",
    "class GlobalAttention(hk.Module):\n",
    "  \"\"\"Global attention.\n",
    "\n",
    "  Jumper et al. (2021) Suppl. Alg. 19 \"MSAColumnGlobalAttention\"\n",
    "  \"\"\"\n",
    "\n",
    "  def __call__(self, msa_act, msa_mask, is_training=False):\n",
    "    # Compute mean query\n",
    "    q = hk.Linear(self.num_head * self.key_dim)(q_avg)\n",
    "    \n",
    "    # K, V from all sequences\n",
    "    k = hk.Linear(self.num_head * self.key_dim)(msa_act)\n",
    "    v = hk.Linear(self.num_head * self.value_dim)(msa_act)\n",
    "    \n",
    "    # Global attention computation\n",
    "    # ...\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
