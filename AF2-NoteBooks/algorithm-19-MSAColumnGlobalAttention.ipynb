{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 19: MSA Column Global Attention\n",
    "\n",
    "MSA Column Global Attention is an efficient attention mechanism for processing large MSAs. Instead of full O(N²) attention, it uses a global query that summarizes all sequences, reducing complexity to O(N).\n",
    "\n",
    "## Algorithm Pseudocode\n",
    "\n",
    "![MSAColumnGlobalAttention](../imgs/algorithms/MSAColumnGlobalAttention.png)\n",
    "\n",
    "## Source Code Location\n",
    "- **File**: `AF2-source-code/model/modules.py`\n",
    "- **Class**: `GlobalAttention`\n",
    "- **Lines**: 717-795"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### Comparison: Regular vs Global Attention\n",
    "\n",
    "| Aspect | Regular Column (Alg 8) | Global Column (Alg 19) |\n",
    "|--------|------------------------|------------------------|\n",
    "| Complexity | O(N_seq²) | O(N_seq) |\n",
    "| Query | All sequences | Mean of sequences |\n",
    "| Memory | O(N² × N_res) | O(N × N_res) |\n",
    "| Use case | Main MSA (~512) | Extra MSA (~5000) |\n",
    "\n",
    "### Algorithm Steps\n",
    "\n",
    "1. Compute global query as mean over sequences\n",
    "2. Compute keys and values for all sequences\n",
    "3. Attend from global query to all sequences\n",
    "4. Broadcast attended result to all sequences\n",
    "5. Apply gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, axis=-1, eps=1e-5):\n",
    "    \"\"\"Layer normalization.\"\"\"\n",
    "    mean = np.mean(x, axis=axis, keepdims=True)\n",
    "    var = np.var(x, axis=axis, keepdims=True)\n",
    "    return (x - mean) / np.sqrt(var + eps)\n",
    "\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    \"\"\"Softmax.\"\"\"\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid.\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-np.clip(x, -20, 20)))\n",
    "\n",
    "\n",
    "def msa_column_global_attention(m, msa_mask=None, num_heads=8, c=8):\n",
    "    \"\"\"\n",
    "    MSA Column Global Attention - Algorithm 19.\n",
    "    \n",
    "    Efficient attention using mean query.\n",
    "    \n",
    "    Args:\n",
    "        m: MSA representation [N_seq, N_res, c_m]\n",
    "        msa_mask: Valid sequence mask [N_seq, N_res]\n",
    "        num_heads: Number of attention heads\n",
    "        c: Head dimension\n",
    "    \n",
    "    Returns:\n",
    "        Updated m [N_seq, N_res, c_m]\n",
    "    \"\"\"\n",
    "    N_seq, N_res, c_m = m.shape\n",
    "    \n",
    "    print(f\"MSA Column Global Attention\")\n",
    "    print(f\"=\"*50)\n",
    "    print(f\"Input: [{N_seq}, {N_res}, {c_m}]\")\n",
    "    print(f\"Heads: {num_heads}, Head dim: {c}\")\n",
    "    \n",
    "    if msa_mask is None:\n",
    "        msa_mask = np.ones((N_seq, N_res), dtype=np.float32)\n",
    "    \n",
    "    # Step 1: Layer normalization\n",
    "    m_norm = layer_norm(m, axis=-1)\n",
    "    print(f\"\\nStep 1 - Layer norm: {m_norm.shape}\")\n",
    "    \n",
    "    # Step 2: Initialize weights\n",
    "    W_q = np.random.randn(c_m, num_heads, c) * (c_m ** -0.5)\n",
    "    W_k = np.random.randn(c_m, num_heads, c) * (c_m ** -0.5)\n",
    "    W_v = np.random.randn(c_m, num_heads, c) * (c_m ** -0.5)\n",
    "    W_g = np.random.randn(c_m, num_heads, c) * (c_m ** -0.5)\n",
    "    W_o = np.random.randn(num_heads, c, c_m) * ((num_heads * c) ** -0.5)\n",
    "    \n",
    "    # Step 3: GLOBAL QUERY - mean over sequences (Line 2)\n",
    "    # Key difference from regular attention!\n",
    "    mask_expanded = msa_mask[:, :, None]  # [N_seq, N_res, 1]\n",
    "    m_masked = m_norm * mask_expanded\n",
    "    \n",
    "    # Mean over valid sequences for each position\n",
    "    n_valid = mask_expanded.sum(axis=0)  # [N_res, 1]\n",
    "    q_input = m_masked.sum(axis=0) / np.maximum(n_valid, 1.0)  # [N_res, c_m]\n",
    "    \n",
    "    # Project to query\n",
    "    q = np.einsum('rc,chd->rhd', q_input, W_q)  # [N_res, H, c]\n",
    "    print(f\"Step 3 - Global query: {q.shape}\")\n",
    "    \n",
    "    # Step 4: Keys and values from all sequences (Lines 3-4)\n",
    "    k = np.einsum('src,chd->srhd', m_norm, W_k)  # [N_seq, N_res, H, c]\n",
    "    v = np.einsum('src,chd->srhd', m_norm, W_v)\n",
    "    print(f\"Step 4 - K: {k.shape}, V: {v.shape}\")\n",
    "    \n",
    "    # Step 5: Gating (per sequence) (Line 5)\n",
    "    g = sigmoid(np.einsum('src,chd->srhd', m_norm, W_g))\n",
    "    print(f\"Step 5 - Gates: {g.shape}\")\n",
    "    \n",
    "    # Step 6: Attention (Lines 6-7)\n",
    "    # Global query attends to all sequences\n",
    "    # q: [N_res, H, c], k: [N_seq, N_res, H, c]\n",
    "    attn_logits = np.einsum('rhd,srhd->rhs', q, k) / np.sqrt(c)  # [N_res, H, N_seq]\n",
    "    \n",
    "    # Apply mask\n",
    "    mask_attn = msa_mask.T[:, None, :]  # [N_res, 1, N_seq]\n",
    "    attn_logits = np.where(mask_attn > 0, attn_logits, -1e9)\n",
    "    \n",
    "    # Softmax over sequences\n",
    "    attn_weights = softmax(attn_logits, axis=-1)\n",
    "    print(f\"Step 6 - Attention weights: {attn_weights.shape}\")\n",
    "    \n",
    "    # Step 7: Apply attention to values (Line 8)\n",
    "    # attended[r,h,d] = sum_s attn[r,h,s] * v[s,r,h,d]\n",
    "    attended = np.einsum('rhs,srhd->rhd', attn_weights, v)\n",
    "    print(f\"Step 7 - Attended (global): {attended.shape}\")\n",
    "    \n",
    "    # Step 8: Broadcast to all sequences (Line 9)\n",
    "    attended_broadcast = np.broadcast_to(\n",
    "        attended[None, :, :, :], \n",
    "        (N_seq, N_res, num_heads, c)\n",
    "    )\n",
    "    print(f\"Step 8 - Broadcast: {attended_broadcast.shape}\")\n",
    "    \n",
    "    # Step 9: Apply gating (Line 10)\n",
    "    gated = g * attended_broadcast\n",
    "    print(f\"Step 9 - Gated: {gated.shape}\")\n",
    "    \n",
    "    # Step 10: Output projection (Line 11)\n",
    "    output = np.einsum('srhd,hdc->src', gated, W_o)\n",
    "    print(f\"Step 10 - Output: {output.shape}\")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Basic functionality\n",
    "print(\"Test 1: Basic Functionality\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "N_seq, N_res, c_m = 1024, 32, 64\n",
    "m = np.random.randn(N_seq, N_res, c_m).astype(np.float32)\n",
    "\n",
    "output = msa_column_global_attention(m, num_heads=8, c=8)\n",
    "\n",
    "print(f\"\\nInput shape: {m.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Shape preserved: {output.shape == m.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Complexity comparison\n",
    "print(\"\\nTest 2: Complexity Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import time\n",
    "\n",
    "N_res, c_m = 32, 64\n",
    "\n",
    "print(\"Global Attention (O(N)):\")\n",
    "for N_seq in [256, 512, 1024, 2048, 4096]:\n",
    "    m = np.random.randn(N_seq, N_res, c_m).astype(np.float32)\n",
    "    \n",
    "    start = time.time()\n",
    "    _ = msa_column_global_attention(m, num_heads=8, c=8)\n",
    "    elapsed = (time.time() - start) * 1000\n",
    "    \n",
    "    print(f\"  N_seq={N_seq:4d}: {elapsed:6.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: With masking\n",
    "print(\"\\nTest 3: With Masking\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "N_seq, N_res, c_m = 512, 24, 32\n",
    "m = np.random.randn(N_seq, N_res, c_m).astype(np.float32)\n",
    "\n",
    "# Mask half of the sequences\n",
    "msa_mask = np.ones((N_seq, N_res), dtype=np.float32)\n",
    "msa_mask[N_seq//2:, :] = 0\n",
    "\n",
    "output = msa_column_global_attention(m, msa_mask=msa_mask, num_heads=8, c=8)\n",
    "\n",
    "print(f\"\\nMasked {(msa_mask == 0).sum() // N_res} sequences\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Global query behavior\n",
    "print(\"\\nTest 4: Global Query Behavior\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "N_seq, N_res, c_m = 256, 16, 32\n",
    "\n",
    "# Create structured MSA with patterns\n",
    "m = np.random.randn(N_seq, N_res, c_m).astype(np.float32)\n",
    "\n",
    "# Make first sequence very different\n",
    "m[0] = m[0] * 10.0\n",
    "\n",
    "output = msa_column_global_attention(m, num_heads=4, c=8)\n",
    "\n",
    "print(f\"First sequence input norm: {np.linalg.norm(m[0]):.2f}\")\n",
    "print(f\"First sequence output norm: {np.linalg.norm(output[0]):.2f}\")\n",
    "print(f\"Other sequences mean output norm: {np.mean([np.linalg.norm(output[i]) for i in range(1, 10)]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification: Key Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verification: Key Properties\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "N_seq, N_res, c_m = 512, 20, 48\n",
    "m = np.random.randn(N_seq, N_res, c_m).astype(np.float32)\n",
    "\n",
    "output = msa_column_global_attention(m, num_heads=8, c=8)\n",
    "\n",
    "# Property 1: Shape preserved\n",
    "shape_preserved = output.shape == m.shape\n",
    "print(f\"Property 1 - Shape preserved: {shape_preserved}\")\n",
    "\n",
    "# Property 2: Finite output\n",
    "output_finite = np.isfinite(output).all()\n",
    "print(f\"Property 2 - Output finite: {output_finite}\")\n",
    "\n",
    "# Property 3: Non-trivial transformation\n",
    "not_identity = not np.allclose(output, m)\n",
    "print(f\"Property 3 - Non-trivial: {not_identity}\")\n",
    "\n",
    "# Property 4: Gating allows sequence-specific output\n",
    "# Different sequences can have different outputs\n",
    "output_seq0 = output[0]\n",
    "output_seq1 = output[1]\n",
    "sequences_differ = not np.allclose(output_seq0, output_seq1)\n",
    "print(f\"Property 4 - Sequence-specific output: {sequences_differ}\")\n",
    "\n",
    "# Property 5: Reasonable scale\n",
    "scale_ratio = output.std() / m.std()\n",
    "print(f\"Property 5 - Scale ratio: {scale_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code Reference\n",
    "\n",
    "```python\n",
    "# From AF2-source-code/model/modules.py\n",
    "\n",
    "class GlobalAttention(hk.Module):\n",
    "  \"\"\"Global attention.\n",
    "\n",
    "  Jumper et al. (2021) Suppl. Alg. 19 \"MSAColumnGlobalAttention\"\n",
    "  \"\"\"\n",
    "\n",
    "  def __call__(self, msa_act, msa_mask, is_training=False):\n",
    "    c = self.config\n",
    "    \n",
    "    # Layer norm\n",
    "    msa_act = common_modules.LayerNorm(...)(msa_act)\n",
    "    \n",
    "    # Compute mean query\n",
    "    q_avg = utils.mask_mean(msa_mask[..., None], msa_act, axis=-3)\n",
    "    q = common_modules.Linear(self.num_head * self.key_dim)(q_avg)\n",
    "    \n",
    "    # K, V from all sequences\n",
    "    k = common_modules.Linear(self.num_head * self.key_dim)(msa_act)\n",
    "    v = common_modules.Linear(self.num_head * self.value_dim)(msa_act)\n",
    "    \n",
    "    # Gating per sequence\n",
    "    g = jax.nn.sigmoid(\n",
    "        common_modules.Linear(self.num_head * self.value_dim)(msa_act))\n",
    "    \n",
    "    # Global attention computation\n",
    "    logits = jnp.einsum('qhc,khc->hqk', q, k)\n",
    "    weights = jax.nn.softmax(logits)\n",
    "    weighted_avg = jnp.einsum('hqk,khc->qhc', weights, v)\n",
    "    \n",
    "    # Gate and project\n",
    "    output = g * weighted_avg[None]  # Broadcast to all sequences\n",
    "    output = common_modules.Linear(self.output_dim)(output)\n",
    "    \n",
    "    return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "1. **O(N) Complexity**: The global query reduces attention complexity from O(N²) to O(N), enabling processing of very large MSAs.\n",
    "\n",
    "2. **Mean Query**: Using the mean as the query represents a \"consensus\" that attends to all sequences.\n",
    "\n",
    "3. **Gating for Diversity**: The per-sequence gating allows different sequences to receive different weighted combinations of the attended values.\n",
    "\n",
    "4. **Broadcasting**: The same attended result is broadcast to all sequences, then modulated by sequence-specific gates.\n",
    "\n",
    "5. **Memory Efficiency**: No need to store O(N²) attention matrices, only O(N) keys/values.\n",
    "\n",
    "6. **Trade-off**: Less expressive than full attention, but sufficient for extracting coarse co-evolution signals from large MSAs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
