{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 8: MSA Column-wise Gated Self-Attention\n",
    "\n",
    "MSA Column Attention applies self-attention across all sequences at each residue position. This allows the model to learn evolutionary patterns by comparing how different sequences vary at the same position.\n",
    "\n",
    "## Algorithm Pseudocode\n",
    "\n",
    "![MSA Column Attention](../imgs/algorithms/MSAColumnAttention.png)\n",
    "\n",
    "## Source Code Location\n",
    "- **File**: `AF2-source-code/model/modules.py`\n",
    "- **Class**: `MSAColumnAttention`\n",
    "- **Lines**: 779-831"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Difference from Row Attention\n",
    "\n",
    "| Aspect | Row Attention (Alg 7) | Column Attention (Alg 8) |\n",
    "|--------|----------------------|-------------------------|\n",
    "| Attention direction | Along residues (within sequence) | Along sequences (at each position) |\n",
    "| Input shape | [N_seq, N_res, c_m] | [N_seq, N_res, c_m] |\n",
    "| Attention axis | Axis 1 (N_res) | Axis 0 (N_seq) |\n",
    "| Pair bias | Yes (from z) | No |\n",
    "| Purpose | Intra-sequence relationships | Cross-sequence (evolutionary) patterns |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, axis=-1, eps=1e-5):\n",
    "    mean = np.mean(x, axis=axis, keepdims=True)\n",
    "    var = np.var(x, axis=axis, keepdims=True)\n",
    "    return (x - mean) / np.sqrt(var + eps)\n",
    "\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "\n",
    "def gated_attention(q_data, m_data, bias, num_head, with_gating=True):\n",
    "    \"\"\"\n",
    "    Standard multi-head attention with optional gating.\n",
    "    \n",
    "    Args:\n",
    "        q_data: Queries [batch, N_queries, c]\n",
    "        m_data: Keys/Values [batch, N_keys, c]\n",
    "        bias: Attention bias [batch, 1, 1, N_keys]\n",
    "        num_head: Number of attention heads\n",
    "        with_gating: Whether to apply output gating\n",
    "    \n",
    "    Returns:\n",
    "        Attention output [batch, N_queries, c]\n",
    "    \"\"\"\n",
    "    batch, n_q, c = q_data.shape\n",
    "    _, n_k, _ = m_data.shape\n",
    "    \n",
    "    head_dim = c // num_head\n",
    "    \n",
    "    # Q, K, V projections\n",
    "    q_w = np.random.randn(c, num_head, head_dim) * 0.01\n",
    "    k_w = np.random.randn(c, num_head, head_dim) * 0.01\n",
    "    v_w = np.random.randn(c, num_head, head_dim) * 0.01\n",
    "    \n",
    "    q = np.einsum('bqa,ahc->bqhc', q_data, q_w) * (head_dim ** -0.5)\n",
    "    k = np.einsum('bka,ahc->bkhc', m_data, k_w)\n",
    "    v = np.einsum('bka,ahc->bkhc', m_data, v_w)\n",
    "    \n",
    "    # Attention logits\n",
    "    logits = np.einsum('bqhc,bkhc->bhqk', q, k) + bias\n",
    "    \n",
    "    # Softmax\n",
    "    weights = softmax(logits, axis=-1)\n",
    "    \n",
    "    # Weighted sum\n",
    "    weighted_avg = np.einsum('bhqk,bkhc->bqhc', weights, v)\n",
    "    \n",
    "    # Gating\n",
    "    if with_gating:\n",
    "        gate_w = np.random.randn(c, num_head, head_dim) * 0.01\n",
    "        gate_b = np.ones((num_head, head_dim))\n",
    "        gate = sigmoid(np.einsum('bqc,chv->bqhv', q_data, gate_w) + gate_b)\n",
    "        weighted_avg = weighted_avg * gate\n",
    "    \n",
    "    # Output projection\n",
    "    o_w = np.random.randn(num_head, head_dim, c) * 0.01\n",
    "    output = np.einsum('bqhc,hco->bqo', weighted_avg, o_w)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msa_column_attention(msa_act, msa_mask, num_head=8):\n",
    "    \"\"\"\n",
    "    MSA Column-wise Gated Self-Attention.\n",
    "    \n",
    "    Algorithm 8 from AlphaFold2 supplementary materials.\n",
    "    \n",
    "    At each residue position, attention is applied across all sequences.\n",
    "    \n",
    "    Args:\n",
    "        msa_act: MSA activations [N_seq, N_res, c_m]\n",
    "        msa_mask: MSA mask [N_seq, N_res]\n",
    "        num_head: Number of attention heads\n",
    "    \n",
    "    Returns:\n",
    "        Updated MSA activations [N_seq, N_res, c_m]\n",
    "    \"\"\"\n",
    "    N_seq, N_res, c_m = msa_act.shape\n",
    "    \n",
    "    print(f\"Input shape: [{N_seq}, {N_res}, {c_m}]\")\n",
    "    \n",
    "    # Step 1: Transpose to make sequences the \"query\" dimension\n",
    "    # Original: [N_seq, N_res, c_m]\n",
    "    # After swap: [N_res, N_seq, c_m]\n",
    "    msa_act_t = np.swapaxes(msa_act, 0, 1)  # [N_res, N_seq, c_m]\n",
    "    msa_mask_t = np.swapaxes(msa_mask, 0, 1)  # [N_res, N_seq]\n",
    "    \n",
    "    print(f\"After transpose: [{msa_act_t.shape[0]}, {msa_act_t.shape[1]}, {msa_act_t.shape[2]}]\")\n",
    "    \n",
    "    # Step 2: Create attention bias from mask\n",
    "    # Masked positions get large negative bias\n",
    "    bias = (1e9 * (msa_mask_t - 1.))[:, None, None, :]  # [N_res, 1, 1, N_seq]\n",
    "    \n",
    "    # Step 3: Layer normalization\n",
    "    msa_act_t = layer_norm(msa_act_t, axis=-1)\n",
    "    \n",
    "    # Step 4: Apply attention\n",
    "    # Each residue position is a \"batch\", attending across sequences\n",
    "    output = gated_attention(\n",
    "        q_data=msa_act_t,\n",
    "        m_data=msa_act_t,\n",
    "        bias=bias,\n",
    "        num_head=num_head,\n",
    "        with_gating=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Attention output: {output.shape}\")\n",
    "    \n",
    "    # Step 5: Transpose back to original shape\n",
    "    output = np.swapaxes(output, 0, 1)  # [N_seq, N_res, c_m]\n",
    "    \n",
    "    print(f\"Final output: {output.shape}\")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parameters\n",
    "N_seq = 128    # Number of sequences in MSA\n",
    "N_res = 64     # Number of residues\n",
    "c_m = 256      # MSA channel dimension\n",
    "\n",
    "# Create test inputs\n",
    "msa_act = np.random.randn(N_seq, N_res, c_m).astype(np.float32)\n",
    "msa_mask = np.ones((N_seq, N_res), dtype=np.float32)\n",
    "\n",
    "# Mask out some sequences (e.g., padding)\n",
    "msa_mask[-10:, :] = 0  # Last 10 sequences are padding\n",
    "\n",
    "print(f\"MSA activations: {msa_act.shape}\")\n",
    "print(f\"MSA mask: {msa_mask.shape}\")\n",
    "print(f\"Valid sequences: {int(msa_mask[:, 0].sum())} / {N_seq}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run column attention\n",
    "output = msa_column_attention(msa_act, msa_mask, num_head=8)\n",
    "\n",
    "print(f\"\\nOutput statistics: mean={output.mean():.6f}, std={output.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Masking Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that masked sequences don't affect output\n",
    "# Run with different masked content, output for valid sequences should be same\n",
    "\n",
    "msa_act_modified = msa_act.copy()\n",
    "msa_act_modified[-10:, :, :] = np.random.randn(10, N_res, c_m) * 100  # Large random values\n",
    "\n",
    "output_modified = msa_column_attention(msa_act_modified, msa_mask, num_head=8)\n",
    "\n",
    "# Compare outputs for valid sequences\n",
    "valid_output_diff = np.abs(output[:-10] - output_modified[:-10]).max()\n",
    "print(f\"Max difference in valid sequence outputs: {valid_output_diff:.6f}\")\n",
    "print(\"(Should be very small if masking works correctly)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code Reference\n",
    "\n",
    "```python\n",
    "# From AF2-source-code/model/modules.py\n",
    "\n",
    "class MSAColumnAttention(hk.Module):\n",
    "  \"\"\"MSA per-column attention.\n",
    "\n",
    "  Jumper et al. (2021) Suppl. Alg. 8 \"MSAColumnAttention\"\n",
    "  \"\"\"\n",
    "\n",
    "  def __call__(self, msa_act, msa_mask, is_training=False):\n",
    "    c = self.config\n",
    "\n",
    "    assert len(msa_act.shape) == 3\n",
    "    assert len(msa_mask.shape) == 2\n",
    "    assert c.orientation == 'per_column'\n",
    "\n",
    "    # Transpose: [N_seq, N_res, c_m] -> [N_res, N_seq, c_m]\n",
    "    msa_act = jnp.swapaxes(msa_act, -2, -3)\n",
    "    msa_mask = jnp.swapaxes(msa_mask, -1, -2)\n",
    "\n",
    "    bias = (1e9 * (msa_mask - 1.))[:, None, None, :]\n",
    "\n",
    "    msa_act = hk.LayerNorm(axis=[-1], ...)(msa_act)\n",
    "\n",
    "    attn_mod = Attention(c, self.global_config, msa_act.shape[-1])\n",
    "    msa_act = mapping.inference_subbatch(\n",
    "        attn_mod, ...,\n",
    "        batched_args=[msa_act, msa_act, bias],\n",
    "        nonbatched_args=[])\n",
    "\n",
    "    # Transpose back: [N_res, N_seq, c_m] -> [N_seq, N_res, c_m]\n",
    "    msa_act = jnp.swapaxes(msa_act, -2, -3)\n",
    "\n",
    "    return msa_act\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "1. **Evolutionary Information**: Column attention captures how sequences co-vary at each position, which is a strong signal for structural contacts.\n",
    "\n",
    "2. **No Pair Bias**: Unlike row attention, column attention doesn't use pair representation bias - it's purely sequence-based.\n",
    "\n",
    "3. **Transpose Trick**: The same attention module is reused by transposing the data.\n",
    "\n",
    "4. **Efficiency**: For long sequences with few MSA sequences, column attention is more efficient than row attention."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
